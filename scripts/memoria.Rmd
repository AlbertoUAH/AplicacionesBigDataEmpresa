---
title: "Aplicaciones del Big Data en la empresa"
author: "Fernández Hernández, Alberto"
date: "29/04/2021"
output:
  html_document:
    toc: true
    toc_float: true
    number_sections: false
---

__Grupo conformado por__: Biel Arenas Bosch, Benjamín Gálvez Megía, Jose Antonio Doyague Hernández, Alberto Fernández Hernández

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

__Nota__: para la realización de la práctica, y a lo largo del concurso, __se ha tomado como base el _score_ obtenido en el modelo 2 realizado en clase:__

1. Selección de variables numéricas y categóricas (con al menos dos categorías y menos de 1000), además de descartar las variables duplicadas _payment_type_ y _quantity_group_, junto a _recorded_by_ (con una única categoría), obteniendo la siguiente puntuación:

```{r, echo=FALSE}
knitr::kable(data.frame("Train accuracy" = 0.8168687, 
                        "Driven Data" = 0.8128,
                        row.names = c("Num + Cat (> 1 & < 1000) sin duplicados")),
             align = 'c')
```

2. Por otro lado, los _chunks_ con las ejecuciones de los modelos están sin ejecutar, para no ralentizar la elaboración del documento. No obstante, en cada apartado se incluyen tablas con las salidas obtenidas, así como el _score_ del _test_ resultante en _DrivenData_. Además, en el último apartado __se dispone de un anexo con el código final ejecutable__.

Durante toda la práctica los modelos __se han realizado con la misma semilla__: 1234.

# 1. Carga de datos y librerías

Para la realización de la práctica, los ficheros utilizados son los siguientes:

1. _train_values.csv_: __fichero con el conjunto de variables y observaciones con los que entrenar el modelo__.

2. _train_labels.csv_: __fichero con las variables objetivo de cada una de las observaciones en _train_values.csv___:

* _functional_

* _non functional_

* _functional needs repair_

3. _test_values.csv_: __fichero de prueba con el que maximizar las predicciones obtenidas con el correspondiente modelo__

En relación con las librerías empleadas, debemos destacar las siguientes:

```{r}
#--- Librerias empleadas
suppressPackageStartupMessages({
  library(dplyr)           # Manipulacion de datos 
  library(data.table)      # Lectura y escritura de ficheros
  library(ggplot2)         # Representacion grafica
  library(inspectdf)       # EDAs automaticos
  library(ranger)          # randomForest (+ rapido que caret)
  library(forcats)         # Tratamiento de variables categoricas
  library(tictoc)          # Calculo del tiempo de ejecucion
  library(missRanger)      # Imputacion de valores NA
  library(knitr)           # Generacion de documentos y formateo de tablas
  library(kableExtra)      # Formateo de tablas (colores)
  library(gmt)             # Calculo de la distancia geografica
  library(stringi)         # Tratamiento de strings
  library(FeatureHashing)  # Hashing Encoding
  library(dataPreparation) # Target encoding
  library(embed)           # Word-Embeddings
  library(recipes)         # Definicion de recetas (preprocesamiento de datos)
  library(lubridate)       # Tratamiento de fechas
  library(ggrepel)         # Añadir etiquetas (texto) a ggplot
  library(xgboost)         # Modelo XGboost
})

# Nota: junto al resto de paquetes debe añadirse h2o, aunque no se ha incluido inicialmente,
#       dado que sobreescribe algunas funciones como las disponibles en "lubridate"
```
```{r}
#--- Carga de ficheros train y test
dattrainOr    <- fread(file = "./data/train_values.csv", data.table = FALSE )
dattrainLabOr <- fread(file = "./data/train_labels.csv", data.table = FALSE )
dattestOr     <- fread(file = "./data/test_values.csv",  data.table = FALSE )
```

# 2. Desarrollo durante el concurso

__En esta sección se comentará las aportaciones que realicé en el grupo durante el concurso para mejorar la puntuación obtenida en clase,__ dado que cada miembro del grupo realizó sus pruebas de forma independiente, sin poner en común un mismo modelo, selección de variables o _Feature Engineering_.

## 2.1 Análisis de variables existentes
En primer lugar, en base a las funciones disponibles en la librería _inspectdf_, __se extrajo un esquema inicial con las variables existentes en el conjunto de datos y su correspondiente tipo__:

```{r, fig.align='center'}
# Analizamos el tipo de dato en cada variable
show_plot(inspect_types(dattrainOr))
```
En conjunto, nos encontramos con:

1. __27 variables de tipo carácter (categóricas)__
2. __10 variables numéricas (entre _integer_ y _numeric_)__
3. __2 variables lógicas__
4. __1 variable tipo fecha__

Por tanto, del mismo modo que lo realizado en el modelo 2:

1. __Conservamos únicamente las variables numéricas y categóricas con más de uno y menos de 1000 valores diferentes__:

```{r}
#-- Variables categoricas
datcat_df <- dattrainOr %>% select(where(is.character))
```

```{r}
# Mediante un bucle for recuperamos el numero de categorias de cada variable caracter
numlev_df <- data.frame(
                          "vars" = names(datcat_df),
                           "levels" = apply(datcat_df, 2, 
                                  function(x) length(unique(x)))
                          
                        )
# Eliminamos los nombres de fila
rownames(numlev_df) <- NULL

#-- Ordenamos de menor a mayor numero de categorias
numlev_df_ordenado <- numlev_df %>% arrange(levels)

color_me <- which(numlev_df_ordenado$levels < 1000 & 
                    numlev_df_ordenado$levels > 1)
kable(numlev_df_ordenado) %>%
  kable_styling() %>%
  row_spec(color_me, bold = TRUE, background = "#4D934D")
```

Por tanto, escogemos desde la variable _source_class_ hasta _lga_, dado que _recorded_by_ solo presenta una única categoría:

```{r}
#-- Conservamos variables con categorias > 1 & < 1000
vars_gd <- numlev_df %>%
  filter(levels < 1000, levels > 1) %>% 
  select(vars)
datcat_gd <- datcat_df[ , vars_gd$vars]
```

```{r}
#-- Variables numericas
datnum_df <- dattrainOr %>% select(where(is.numeric))
```

```{r}
# Unificamos ambos tipos de variables...
datnumcat_df <- cbind(datnum_df, datcat_gd)
```

```{r}
# ...Como tambien la variable objetivo
dattrainOrlab <- merge(
  datnumcat_df, dattrainLabOr,
  by.x = c('id'), by.y = c('id'),
  sort = FALSE
)
```

__El resto de variables las almacenamos en un _data.table_ auxiliar__, con el objetivo de emplearlas en la sección 3:

```{r}
#-- Almacenamos variables categoricas sin utilizar
datcat_mas_1000_mas_logicas_dr <- data.table(
  funder         = c(dattrainOr$funder,         dattestOr$funder         ),
  ward           = c(dattrainOr$ward,           dattestOr$ward           ),
  installer      = c(dattrainOr$installer,      dattestOr$installer      ),
  scheme_name    = c(dattrainOr$scheme_name,    dattestOr$scheme_name    ),
  subvillage     = c(dattrainOr$subvillage,     dattestOr$subvillage     ),
  wpt_name       = c(dattrainOr$wpt_name,       dattestOr$wpt_name       ),
  permit         = c(dattrainOr$permit,         dattestOr$permit         ),
  public_meeting = c(dattrainOr$public_meeting, dattestOr$public_meeting ),
  date_recorded  = c(dattrainOr$date_recored,   dattestOr$date_recorded  )
)
```

2. __Eliminación de variables con información duplicada__, concretamente _payment_type_ y _quantity_group_:

```{r}
#-- payment_type: categorias muy similares con la variable "payment"
table(dattrainOrlab$payment_type)
```
```{r}
table(dattrainOrlab$payment)
```
```{r}
dattrainOrlab$payment_type   <- NULL
```

```{r}
#-- quantity_group: mismas categorias que con la variable quantity
table(dattrainOrlab$quantity_group)
```

```{r}
table(dattrainOrlab$quantity)
```

```{r}
dattrainOrlab$quantity_group <- NULL
```

## 2.1 Tratamiento de valores anómalos

Una vez pre-procesado el conjunto de entrenamiento, una de las primeras ideas que se tuvo en cuenta fue __el tratamiento de valores anómalos, concretamente a cero, de determinadas variables__, anotados durante la primera fase de análisis exploratorio realizada en clase:

1. _construction_year_

2. _gps_height_: en el caso de la altura GPS, __no se han considerado los valores negativos como valores anómalos__, principalmente porque, trasladando el problema a un entorno real, es posible la existencia de alturas negativas (regiones situadas por debajo del nivel del mar), además de no existir una elevada acumulación (como si sucede con el cero).

3. _longitude_

Sobre dichas variables podemos observar en los siguientes histogramas una concentración "anómala" de valores a cero:

```{r, fig.align = 'center', fig.width=8}
# Graficos de distribucion
# Histograms for numeric columns
show_plot(
  inspect_num(dattrainOrlab[c("construction_year",
                              "gps_height",
                              "longitude")])
  
)
```

Por tanto, para eliminar y posteriormente imputar dichos ceros, __haremos uso de una función denominada impute_zeros__, la cual permite establecer inicialmente a NA los valores a cero, así como su posterior imputación haciendo uso de la librería _missRanger_.

__Nota__: a lo largo de la memoria se definen varias funciones que se emplean comunmente tanto en el concurso como en el desarrollo individual, evitando con ello la redundancia de código.

```{r}
# Funcion para la imputacion de ceros con missRanger
impute_zeros <- function(data, column, k, num_trees, impute = TRUE, seed = 1234) {
  new_feature <- paste0("fe_", column)
  
  data[, new_feature] <- data[, column]
  data[, new_feature] <- ifelse( 
                            data[, new_feature] == 0, 
                            NA, 
                            data[, new_feature]
  )
  original_data       <- data[, new_feature]
  
  data[, column]      <- NULL
  
  if (impute == TRUE) {
    data_imp <- missRanger(
                            data,  
                            pmm.k = k,
                            num.trees = num_trees,
                            seed = seed,
                            verbose = 0
                           )
    # Finalmente, creamos una nueva columna binaria
    # 1 = el valor era NA; 0 = No
    new_feature_na      <- paste0("fe_", column, "_na")
    data_imp[, new_feature_na] <- ifelse(
                                    is.na(original_data),
                                    1,
                                    0
    )
  }
  return(data_imp)
}
```

Una vez definida la funcion, imputamos cada una de las columnas anteriores, __no sin antes eliminar la columna _status_group_, con el objetivo de evitar que la imputación también dependa del valor de la variable objetivo__, empleando k = 5 como número de vecinos:

```{r}
# Almacenamos en un vector la variable objetivo
status_group_vector        <- dattrainOrlab$status_group
dattrainOrlab$status_group <- NULL
```
```{r, eval = FALSE}
#--- construction_year
dattrainOrlab_imp <- impute_zeros(dattrainOrlab,
                                  column = "construction_year",
                                  k = 5,
                                  num_trees = 100)
```

```{r, eval = FALSE}
#--- gps_heigth
dattrainOrlab_imp <- impute_zeros(dattrainOrlab_imp,
                                  column = "gps_height",
                                  k = 5,
                                  num_trees = 100)
```

```{r, eval = FALSE}
#--- longitude
dattrainOrlab_imp <- impute_zeros(dattrainOrlab_imp,
                                  column = "longitude",
                                  k = 5,
                                  num_trees = 100)
```

Tras la imputación, añadimos nuevamente la variable objetivo al conjunto de entrenamiento:

```{r,eval=FALSE}
dattrainOrlab_imp$status_group <- status_group_vector
dattrainOrlab_imp$status_group <- as.factor(dattrainOrlab_imp$status_group)
```

En relación con los datos _test_, dado que no debemos aplicar la imputación de los valores _missing_ a partir de dicho conjunto, a cada valor NA (es decir, igual a cero), se tomó la decisión de __imputarlo por un valor muy diferente al resto de la población__, concretamente a 99.999, __así como añadir una columna binaria adicional que indice si el valor imputado era o no un valor _missing___:

```{r, eval=FALSE}
dattestOr_imp <- dattestOr

#-- Completar valores anomales a 99999 + columnas "_na_"
for(column in c("construction_year", "gps_height", "longitude")) {
  new_feature <- paste0("fe_", column)
  dattestOr_imp[, new_feature] <- ifelse(
                                          dattestOr_imp[, column] == 0,
                                          99999,
                                          dattestOr_imp[, column]
  )
  dattestOr_imp[, column] <- NULL
  
  new_feature_na <- paste0("fe_", column, "_na")
  dattestOr_imp[, new_feature_na] <- ifelse(
                                          dattestOr_imp[, new_feature] == 99999,
                                          1,
                                          0
  ) 
}
```

Una vez imputadas las variables, por medio de la función _ranger_ creamos un primer modelo de _Random Forest_:

```{r}
#-- Funcion que devuelve un modelo Random Forest entrenado
fit_random_forest <- function(formula, data, num_trees = 500, mtry = NULL, seed = 1234) {
  tic()
  my_model <- ranger( 
    formula, 
    importance = 'impurity',
    data       = data,
    num.trees = num_trees,
    mtry = mtry,
    verbose = FALSE,
    seed = seed
  )
  # "Estimacion" del error / acierto "esperado" (OOB accuracy)
  success <- 1 - my_model$prediction.error
  print(success)
  toc()
  
  return(my_model)
}
```

De forma adicional, creamos una función que permita:

1. __Crear un grafico con la importancia de las variables del modelo pasado como argumento__

2. __Almacenar el gráfico en disco__

```{r}
#-- Funcion para pintar importancia de variables (ademas de guardar el grafico)
save_importance_ggplot <- function(model, path, title) {
  impor_df <- as.data.frame(model$variable.importance)
  names(impor_df)[1] <- c('Importance')
  impor_df$vars <- rownames(impor_df)
  rownames(impor_df) <- NULL
  
  print(ggplot(impor_df, aes(fct_reorder(vars, Importance), Importance)) +
      geom_col(group = 1, fill = "darkred") +
      coord_flip() + 
      labs(x = 'Variables', y = 'Importancia', title = title) +
      theme(axis.text.y = element_text(face = "bold", colour = "black"))
    )
  ggsave(path)
}
```

```{r, eval = FALSE}
#-- Entrenamiento del primer modelo
formula    <- as.formula('status_group~.')

my_model_1 <- fit_random_forest(formula, dattrainOrlab_imp)
```

```
[1] 0.8101178
36.403 sec elapsed
```

Tras crear el modelo, guardamos el gráfico con la importancia de cada una de las variables:

```{r, eval=FALSE}
save_importance_ggplot(my_model_1, "./charts/01_num_cat_menos1000_todo_imp.png",
                       title = "Importancia Variables Modelo 01")
```

```{r, echo=FALSE,fig.align="center"}
include_graphics("./charts/01_num_cat_menos1000_todo_imp.png")
```

Analizando el gráfico de importancia, observamos que las variables imputadas (año de construcción, altura GPS y longitud) presentan una elevada importancia. No obstante, las variables "_na" __no parecen aportar una importancia relevante al modelo__. Pese a ello, realizamos las predicciones y comparamos el _score_ obtenido:

```{r}
#-- Funcion para realizar prediccion sobre el modelo pasado como parametro
make_predictions <- function(model, test_data) {
  # Prediccion
  my_pred <- predict(model, test_data)
  
  # Submission
  my_sub <- data.table(
    id = test_data[, "id"],
    status_group = my_pred$predictions
  )
  
  return(my_sub)
}
```
```{r, eval = FALSE}
my_sub_1 <- make_predictions(my_model_1, dattestOr_imp)
# guardo submission
fwrite(my_sub_1, file = "./submissions/01_num_cat_menos1000_todo_imp.csv")
```

```{r, echo=FALSE}
puntuaciones <- data.frame("Train accuracy" = c(0.8168687, 0.8101178), 
                        "Driven Data" = c(0.8128, 0.8096),
                row.names = c("Num + Cat (> 1 & < 1000) sin duplicados",
                "Num + Cat (> 1 & < 1000) sin duplicados longitude-construction_year-gps_height imp."))
color_me <- which(puntuaciones$Driven.Data == 0.8128)
kable(puntuaciones) %>%
  kable_styling() %>%
  row_spec(color_me, bold = TRUE, background = "#4D934D")
```

Como podemos comprobar, el hecho de imputar las variables anteriores no ha mejorado el modelo. Sin embargo, __¿Y si eliminamos las variables "\_na" anteriores?__

```{r, eval=FALSE}
dattrainOrlab_imp_sin_col_na <- dattrainOrlab_imp
# Eliminamos las columnas "_na"
dattrainOrlab_imp_sin_col_na$fe_construction_year_na <- NULL
dattrainOrlab_imp_sin_col_na$fe_gps_height_na        <- NULL
dattrainOrlab_imp_sin_col_na$fe_longitude_na         <- NULL

# Idem con los datos test
dattestOr_imp_sin_col_na    <- dattestOr_imp
# Eliminamos las columnas "_na"
dattestOr_imp_sin_col_na$fe_construction_year_na <- NULL
dattestOr_imp_sin_col_na$fe_gps_height_na        <- NULL
dattestOr_imp_sin_col_na$fe_longitude_na         <- NULL
```

```{r, eval = FALSE}
my_model_1_sin_cols_na <- fit_random_forest(formula, dattrainOrlab_imp_sin_col_na)
```

```
[1] 0.8105724
39.914 sec elapsed
```

En relación con el conjunto _train_, parece mejorar ligeramente (de 0.8103535 a 0.811229). Analicemos el conjunto _test_:

```{r, eval = FALSE}
my_sub_1_sin_cols_na <- make_predictions(my_model_1_sin_cols_na, dattestOr_imp_sin_col_na)
# guardo submission
fwrite(my_sub_1_sin_cols_na, file = "./submissions/01_num_cat_menos1000_todo_imp_sin_cols_na.csv")
```

```{r, echo=FALSE}
puntuaciones <- data.frame("Train accuracy" = c(0.8168687, 0.8101178, 0.8105724), 
                        "Driven Data" = c(0.8128, 0.8096, 0.8081),
                row.names = c("Num + Cat (> 1 & < 1000) sin duplicados",
                        "Num + Cat (> 1 & < 1000) sin duplicados longitude-construction_year-gps_height imp.", 
                        "Anterior, eliminando columnas '_na'"))

color_me <- which(puntuaciones$Driven.Data == 0.8128)
kable(puntuaciones) %>%
  kable_styling() %>%
  row_spec(color_me, bold = TRUE, background = "#4D934D")
```

Podemos comprobar que, pese a eliminar dichas variables, el _score_ del modelo empeora.

## 2.2 Creación de nuevas variables
En vistas a los resultados obtenidos en el apartado anterior, __se decidió no imputar dichas variables y buscar una alternativa a la mejora del modelo__. Para ello, se recurrió a la creación de nuevas variables o _feature engineering_:

1. _construction_year_: __antigüedad del pozo__, restando la fecha más reciente en el _dataset_ (2014) a cada uno de los valores.

2. _longitude_ y _latitude_: __distancia geográfica__. En lugar de la raíz cuadrada de la suma de las coordenadas al cuadrado, _R_ dispone de un paquete denominado _gmt_ con el que calcular la [__distancia geográfica de cada bomba al punto de referencia (0,0)__](https://rdrr.io/cran/gmt/man/geodist.html), en lugar de emplear la distancia Euclídea:

$$
D = cos^{−1}[sin \theta_1 sin \theta_2 + cos\theta_1 cos \theta_2 cos(\phi_1−\phi_2)])
$$
```{r}
#-- Ejemplo
geodist(55.75,37.63, 0, 0)  # Moscow - (0,0)
```

3. _amount_tsh_ y _population_: analizando el gráfico de importancia del primer modelo, se comprobó que variables como la población o la cantidad de agua disponible no son especialmente relevantes. Como consecuencia, se propuso la creación de una nueva variable, basada en la __cantidad de agua que habría disponible por persona en cada pozo__, dividiendo la cantidad total de agua en cada observación ( _amount_tsh_ ) entre el total de población.

```{r}
#---- Feature Engineering
#---  Creacion de nuevas variables
#--   Antiguedad del pozo (2014 - fecha)
# Train
dattrainOrlab$fe_cyear <- 2014 - dattrainOrlab$construction_year

# Test
dattestOr$fe_cyear     <- 2014 - dattestOr$construction_year
```
```{r}
#--   Distancia geografica al punto (0,0)
# geodist (latitud_origen, longitud_origen, latitud_destino, longitud_destino)
# Por defecto, las unidades estan expresadas en kilometros
# Train
dattrainOrlab$fe_dist <- geodist(dattrainOrlab$latitude, dattrainOrlab$longitude, 0, 0)

# Test
dattestOr$fe_dist     <- geodist(dattestOr$latitude, dattestOr$longitude, 0, 0)
```
```{r}
#--  Cantidad de agua disponible por persona
# Train
dattrainOrlab$fe_cant_agua <- ifelse(dattrainOrlab$population == 0,
                                     0,
                                    round(dattrainOrlab$amount_tsh /
                                          dattrainOrlab$population, 3)
                              )

# Test
dattestOr$fe_cant_agua    <- ifelse(dattestOr$population == 0,
                                    0,
                                    round(dattestOr$amount_tsh /
                                          dattestOr$population, 3)
                              )
```

Una vez creadas las variables, entrenamos nuevamente el modelo _random forest_:

```{r}
# Incluimos la variable objetivo
dattrainOrlab$status_group <- status_group_vector
dattrainOrlab$status_group <- as.factor(dattrainOrlab$status_group)
```
```{r, eval = FALSE}
my_model_2 <- fit_random_forest(formula, dattrainOrlab)
```

A simple vista, el valor obtenido en el conjunto de entrenamiento mejora de 0.810 en el caso anterior a 0.812. Veamos la importancia de las variables:

```
[1] 0.8122391
38.009 sec elapsed
```

```{r, eval=FALSE}
save_importance_ggplot(my_model_2, "./charts/02_num_cat_menos1000_fe_cyear_dist_cant_agua.png",
                       title = "Importancia Variables Modelo 02")
```

```{r, echo=FALSE, fig.align='center'}
#- Guardamos el grafico con la importancia del modelo
include_graphics('./charts/02_num_cat_menos1000_fe_cyear_dist_cant_agua.png')
```

De forma general, las variables creadas se sitúan entre las variables con mayor importancia (a excepción de _fe_cant_agua_). A continuación, si analizamos las predicciones obtenidas en el conjunto test:

```{r, eval = FALSE}
my_sub_2 <- make_predictions(my_model_2, dattestOr)
# guardo submission
fwrite(my_sub_2, file = "./submissions/02_num_cat_menos1000_fe_cyear_dist_cant_agua.csv")
```

```{r, echo=FALSE}
puntuaciones <- data.frame("Train accuracy" = c(0.8168687, 0.8101178, 0.8105724, 0.8122391), 
                        "Driven Data" = c(0.8128, 0.8096, 0.8081, 0.8174),
                row.names = c("Num + Cat (> 1 & < 1000) sin duplicados",
                "Num + Cat (> 1 & < 1000) sin duplicados longitude-construction_year-gps_height imp.", 
                "Anterior, eliminando columnas '_na'",
                "Num + Cat (> 1 & < 1000) fe cyear + dist + cant_agua"))
            
color_me <- which(puntuaciones$Driven.Data == 0.8174)
kable(puntuaciones) %>%
  kable_styling() %>%
  row_spec(color_me, bold = TRUE, background = "#4D934D")
```

## 2.3 Inclusión de _date_recorded_

Con la inclusión de las nuevas variables, __el resultado mejoró con respecto a los dos _submission_ anteriores__, por lo que se decidió mantener cada una ellas. En relación con la siguiente prueba, se tomó la decisión de incluir una variable adicional que hasta el momento no se había tenido en cuenta: _date_recorded_. Dado que la variable se encuentra en formato fecha, en lugar de incluir directamente el campo se dividió en un total de tres nuevas variables, gracias al paquete _lubridate_:

1. __Año__
2. __Mes__
3. __Diferencia entre la fecha de observación y la fecha de construcción de la bomba__, de forma que se pueda tener una columna que indique su antigüedad, en relación con su fecha de instalación:

```{r}
#-- date_recorded --> año
#   Train
dattrainOrlab$fe_dr_year <- year(dattrainOr$date_recorded)

#   Test
dattestOr$fe_dr_year     <- year(dattestOr$date_recorded)
```

En relación a la diferencia entre _date_recorded_ y _construction_year_, llamó la atención la __diferencia negativa entre algunas observaciones__, es decir, __bombas de agua en las que la fecha de registro fue posterior a la fecha de su construcción__:

```{r}
#-- date_recorded --> año date_recorded - año construction_date
#   Train
dattrainOrlab$fe_dr_year_cyear_diff <- dattrainOrlab$fe_dr_year - dattrainOrlab$construction_year

#   Test
dattestOr$fe_dr_year_cyear_diff     <- dattestOr$fe_dr_year - dattestOr$construction_year
```

```{r}
#-- date_recorded --> mes
#   Train
dattrainOrlab$fe_dr_month <- month(dattrainOr$date_recorded)

#   Test
dattestOr$fe_dr_month     <- month(dattestOr$date_recorded)
```

```{r}
sum(dattrainOrlab$fe_dr_year_cyear_diff < 0) # Train
sum(dattestOr$fe_dr_year_cyear_diff < 0)     # Test
```

Nuevamente, tras crear las variables entrenamos el modelo _random forest_:

```{r, eval = FALSE}
#-- Modelo 3
my_model_3 <- fit_random_forest(formula, dattrainOrlab)
```

```
[1] 0.8114983
46.337 sec elapsed
```

En relación al conjunto _train_ anterior, ha mejorado considerablemente ¿Y en relación gráfico de importancia?

```{r, eval=FALSE}
save_importance_ggplot(my_model_3, "./charts/03_num_cat_menos1000_fe_cyear_dist_cant_agua_dr_year_month_old.png",
                       title = "Importancia Variables Modelo 03")
```

```{r, echo=FALSE, fig.align='center'}
#- Guardamos el grafico con la importancia del modelo
include_graphics('./charts/03_num_cat_menos1000_fe_cyear_dist_cant_agua_dr_year_month_old.png')
```

Salvo la variable _fe_dr_year_ (año de _date_recorded_), por lo general son variables relevantes. Además, si analizamos el conjunto _test_:

```{r, eval=FALSE}
my_sub_3 <- make_predictions(my_model_3, dattestOr)
# guardo submission
fwrite(my_sub_3, 
    file = "./submissions/03_num_cat_menos1000_fe_cyear_dist_cant_agua_dr_year_month_old.csv")
```

```{r, echo=FALSE}
puntuaciones <- data.frame("Train accuracy" = c(0.8168687, 0.8101178, 0.8105724, 0.8122391, 0.8114983), 
                        "Driven Data" = c(0.8128, 0.8096, 0.8081, 0.8174, 0.8176),
            row.names = c("Num + Cat (> 1 & < 1000) sin duplicados",
            "Num + Cat (> 1 & < 1000) sin duplicados longitude-construction_year-gps_height imp.", 
            "Anterior, eliminando columnas '_na'",
            "Num + Cat (> 1 & < 1000) fe cyear + dist + cant_agua",
            "Num + Cat (> 1 & < 1000) fe cyear + dist + cant_agua + dr_year + dr_month + dr_year_cyear_diff"))

color_me <- which(puntuaciones$Driven.Data == 0.8176)
kable(puntuaciones) %>%
  kable_styling() %>%
  row_spec(color_me, bold = TRUE, background = "#4D934D")
```

Observamos que __el modelo ha mejorado a 0.8176__. Sin embargo, no todas las variables son de especial importancia, como es el caso de _fe_dr_year_ (ver gráfica anterior) ¿Y si eliminamos dicha variable?

```{r}
dattrainOrlab$fe_dr_year <- NULL
dattestOr$fe_dr_year     <- NULL
```

```{r, eval=FALSE}
# Calculamos el nuevo modelo
my_model_3_sin_dr_year <- fit_random_forest(formula, dattrainOrlab)
```

```
[1] 0.8124579
41.523 sec elapsed
```

```{r, eval=FALSE}
my_sub_3_sin_dr_year <- make_predictions(my_model_3_sin_dr_year, dattestOr)
# guardo submission
fwrite(my_sub_3_sin_dr_year, 
    file = "./submissions/03_num_cat_menos1000_fe_cyear_dist_cant_agua_month_old.csv")
```

```{r, echo=FALSE}
puntuaciones <- data.frame("Train accuracy" = c(0.8168687, 0.8101178, 0.8105724, 0.8122391, 0.8114983, 0.8124579), 
                        "Driven Data" = c(0.8128, 0.8096, 0.8081, 0.8174, 0.8176, 0.8176),
                row.names = c("Num + Cat (> 1 & < 1000) sin duplicados",
                "Num + Cat (> 1 & < 1000) sin duplicados longitude-construction_year-gps_height imp.", 
                "Anterior, eliminando columnas '_na'",
                "Num + Cat (> 1 & < 1000) fe cyear + dist + cant_agua",
                "Num + Cat (> 1 & < 1000) fe cyear + dist + cant_agua + dr_year + dr_month + dr_year_cyear_diff",
                "Num + Cat (> 1 & < 1000) fe cyear + dist + cant_agua + dr_month + dr_year_cyear_diff "))

color_me <- which(puntuaciones$Driven.Data == 0.8176)
kable(puntuaciones) %>%
  kable_styling() %>%
  row_spec(color_me, bold = TRUE, background = "#4D934D")
```

```{r}
#-- Guardamos el mejor dataset para la mejora del modelo tras el concurso
fwrite(dattrainOrlab, "./data/train_values_concurso.csv")
fwrite(dattestOr    , "./data/test_values_concurso.csv" )
```

Curiosamente, eliminando el año de _date_recorded_ no hace empeorar el _score_, __por lo que se decidió no tener en cuenta dicha variable, al no aportar mejoría alguna__.

## 2.4 Tuneo modelo final

Como última _submission realizada en clase_ __se realizó un tuneo de hiperparámetros del modelo _random forest_, empleando todos los parámetros utilizados hasta el momento.__

```{r, eval=FALSE}
#-- Tuneo del numero de arboles y mtry
my_ntree <- c(500, 600, 700, 800)
my_mtry  <- c(5, 6, 7)
my_pars  <- expand.grid(my_ntree, my_mtry)
names(my_pars) <- c('myntree', 'mymtry')
my_pars$accuracy <- 0

for (i in 1:nrow(my_pars)) {
   my_model_tuning <- fit_random_forest(formula, dattrainOrlab,
                                        num_trees = my_pars$myntree[[i]],
                                        mtry = my_pars$mymtry[i])
   my_pars$accuracy[i] <- (1 - my_model_tuning$prediction.error)
}
rm(my_model_tuning)
```

Tras realizar el tuneo de hiperparámetros, comprobamos qué modelo obtiene un mayor _accuracy_:

```{r}
load("./rdata/00.RData")
my_pars[which(my_pars$accuracy == max(my_pars$accuracy)), ]
```

```{r, eval=FALSE}
#-- Entrenamos el modelo con 800 arboles y mtry = 7
my_model_4 <- fit_random_forest(formula, dattrainOrlab, num_trees = 800, mtry = 7)
```

```
[1] 0.8135017
88.765 sec elapsed
```

Finalmente, analizamos el _score_ de las predicciones obtenidas:

```{r, eval=FALSE}
my_sub_4 <- make_predictions(my_model_4, dattestOr)
# guardo submission
fwrite(my_sub_4, 
    file = "./submissions/04_num_cat_menos1000_fe_tunned.csv")
```

```{r, echo=FALSE}
puntuaciones <- data.frame("Train accuracy" = c(0.8168687, 0.8101178, 0.8105724, 0.8122391, 0.8114983, 0.8124579, 0.8135017), 
                        "Driven Data" = c(0.8128, 0.8096, 0.8081, 0.8174, 0.8176, 0.8176, 0.8174),
                row.names = c("Num + Cat (> 1 & < 1000) sin duplicados",
                "Num + Cat (> 1 & < 1000) sin duplicados imp", 
                "Num + Cat (> 1 & < 1000) sin duplicados imp sin cols na",
                "Num + Cat (> 1 & < 1000) fe cyear + dist + cant_agua",
                "Num + Cat (> 1 & < 1000) fe cyear + dist + cant_agua + dr_year + dr_month + dr_year_cyear_diff",
                "Num + Cat (> 1 & < 1000) fe cyear + dist + cant_agua + dr_month + dr_year_cyear_diff", 
                "Num + Cat (> 1 & < 1000) fe + tunning random forest"))

color_me <- which(puntuaciones$Driven.Data == 0.8176)
kable(puntuaciones) %>%
  kable_styling() %>%
  row_spec(color_me, bold = TRUE, background = "#4D934D")
```

Realizando el tuneo del _Random Forest_, el _score_ del modelo no consigue mejorar.

# 3. Mejora del modelo tras el concurso

## 3.1 Feature Engineering

Tras el concurso, por cuenta propia __se ha decidido partir del mejor modelo obtenido hasta el momento__, correspondiente con:

1. __Variables numéricas__
2. __Variables categóricas (> 1 y < 1000 categorías)__
3. ___Feature Engineering_: _cyear_, _dist_, _cant_agua_, _dr_month_ y _dr_year_cyear_diff___
4. __Empleando un modelo _Random Forest_ estándar: 500 árboles y mtry = 6__ (raíz cuadrada del número de variables)

Sobre dicho modelo, se ha obtenido una mejor puntuación de 0.8124579 en el conjunto _train_ y 0.8176 en el conjunto de prueba, por lo que el objetivo consistirá en superar dicho _accuracy_, así como la mejor puntuación obtenida por el grupo ganador (0.8248).

```{r, echo=FALSE}
rm(list = setdiff(ls(), "datcat_mas_1000_mas_logicas_dr"))
```

__Importante__: con el objetivo de evitar redundancia de código, el conjunto de datos obtenido en la fase anterior se ha almacenado en ficheros csv denominados _train_values_concurso.csv_ y _test_values_concurso.csv_. Además, de ahora en adelante se trabajará con _data.table_ en lugar de _data.frames_, con fines únicamente didácticos (aprender una nueva estructura de datos):

```{r}
# Cargamos el conjunto de datos con el que se ha obtenido un mejor score en clase
dattrainOrlab    <- fread(file = "./data/train_values_concurso.csv", data.table = FALSE )
dattestOr        <- fread(file = "./data/test_values_concurso.csv",  data.table = FALSE )
```

Una vez cargados ambos datos (entrenamiento y prueba), se tomó la decisión de juntar ambos ficheros en una única variable (dat_completo), evitando de este modo la duplicidad en el código. Por tanto, y dado que el conjunto _test_ no dispone de la columna _status_group_, almacenamos la variable objetivo del conjunto _train_ en un vector:

```{r}
vector_status_group        <- dattrainOrlab$status_group
dattrainOrlab$status_group <- NULL

# Creamos el dataset completo (train y test unidos)
columnas_test  <- names(dattestOr)[names(dattestOr) %in% names(dattrainOrlab)]
datcompleto    <- rbind(dattrainOrlab, dattestOr[, columnas_test])

#-- Nos traemos funder, ward (menos de 2100 categorias)
#   Para el siguiente apartado
datcompleto$funder <- datcat_mas_1000_mas_logicas_dr$funder
datcompleto$ward   <- datcat_mas_1000_mas_logicas_dr$ward

# Lo convertimos a data.table
datcompleto <- as.data.table(datcompleto)
```

Además, de cara a la posterior separación del conjunto _train_ y _test_ para el entrenamiento, almacenamos en una variable el número de fila __a partir del cual comienza el conjunto _test___:

```{r}
# El conjunto test empieza a partir de la fila n. 59401
# 50785: primer id del conjunto test
fila_test <- which(datcompleto$id == 50785)
```

### 3.1.1 Variables < 2100 valores únicos

Hasta ahora, hemos incluido variables con menos de 1000 categorias únicas, esto es, hasta la variable _lga_. Durante el concurso, no se tuvieron en cuenta, dado que el objetivo fue __maximizar el _score_ del modelo sin necesidad de recurrir a variables con un alto número de categorías__. Por ello, de cara a la parte individual se tomó la primera decisión de trabajar sobre dichos campos.

Con el objetivo de seguir un orden y __no incluir todas las variables "de forma abrupta"__, comenzamos con aquellas con menos de 2100 categorías, esto es, _funder_ y _ward_:

```{r}
#-- Niveles de las categoricas.
datcat_df <- as.data.frame(datcompleto %>% select(where(is.character)))

numlev_df <- data.frame()
for (i in 1:ncol(datcat_df)) {
  col_tmp <- datcat_df[, i]
  num_lev <- length(unique(col_tmp))
  numlev_df[i, 1] <- names(datcat_df)[i]
  numlev_df[i, 2] <- num_lev
}
names(numlev_df) <- c('vars', 'levels')
numlev_df %>% arrange(levels)
```

Dado que existen demasiadas categorías, podemos preguntarnos si realmente existen 2098 y 2141 valores unicos en _ward_ y _funder_, respectivamente. De hecho, podemos comprobar como algunas de las categorías se diferencian por tan solo una o dos palabras:

```
Lgcdg   - Lgcbg  # En el caso de funder
Magole  - Magoma # En el caso de ward
```

Es decir, se tratan de regiones muy similares desde un punto de vista léxico. Por tanto, ¿quién nos asegura que dos o más categorías se diferencien por tan solo un espacio en blanco, una palabra en mayúscula o por signos de puntuación? Como consecuencia, antes de procesar un modelo se procedió a __realizar una limpieza sobre ambas columnas, con el propósito de comprobar si se reduce el número de valores únicos__. Para ello, empleamos la librería __stringi__, creando una función que permita filtrar:

1. Mayúsculas
2. Espacios en blanco
3. Signos de puntuación

```{r}
#-- Funcion para limpieza de strings (vectorizado)
clean_text <- function(text) {
  stri_trans_tolower(
    stri_replace_all_regex(
      text, 
      pattern = "[ +\\p{Punct}]", 
      replacement = ""
      )
    )
}
```

Una vez creada la función, lo aplicamos sobre el _data.table_:

```{r}
datcompleto[, fe_funder := clean_text(funder)][, fe_ward := clean_text(ward)]
```

Tras aplicar dicha función, comprobamos si el número de categorías se ha visto reducido:

```{r}
datcat_df <- as.data.frame(datcompleto %>% select(where(is.character)))

numlev_df <- data.frame()
for (i in 1:ncol(datcat_df)) {
  col_tmp <- datcat_df[, i]
  num_lev <- length(unique(col_tmp))
  numlev_df[i, 1] <- names(datcat_df)[i]
  numlev_df[i, 2] <- num_lev
}
names(numlev_df) <- c('vars', 'levels')
numlev_df %>% arrange(levels)
```

Analizando la salida anterior, podemos comprobar como el número de categorías __se ha visto reducido ligeramente en ambas columnas__:

1. _ward_  : de 2098 a 2096
2. _funder_: de 2141 a 2110

Una vez creadas las nuevas columnas, eliminamos las originales y lanzamos el modelo:

```{r}
datcompleto[, c("ward", "funder") := NULL]
```

```{r, eval=FALSE}
#-- Modelo
# Dividimos entre conjunto de entrenamiento y prueba
train <- datcompleto[c(1:fila_test-1),]
train$status_group <- vector_status_group
train$status_group <- as.factor(train$status_group)

test <- datcompleto[c(fila_test:nrow(datcompleto)),]

formula   <- as.formula("status_group~.")

# Empleamos la funcion fit_random_forest definida anteriormente
my_model_5 <- fit_random_forest(formula, train)
```

```
[1] 0.8149832
39.414 sec elapsed
```

Tras crear el modelo, echemos un vistazo a la importancia de las variables:

```{r, echo=FALSE}
include_graphics('./charts/05_04_mas_fe_inst_funder_scheme_ward.png')
```

Observamos que ambas columnas se sitúan entre las variables con mayor relevancia (importancia en torno a 1000). Veamos si con el conjunto _test_ obtenemos un mejor _score_:

```{r, eval=FALSE}
my_sub_5 <- make_predictions(my_model_5, dattestOr)
# guardo submission
fwrite(my_sub_5, 
    file = "./submissions/05_04_mas_fe_inst_funder_scheme_ward.csv")
```

```{r, echo=FALSE}
puntuaciones <- data.frame("Train accuracy" = c('-', 0.8149832), 
                        "Driven Data" = c(0.8176, 0.8197),
                        row.names = c("Mejor accuracy en el concurso",
                                      "Num + Cat (> 1 & < 2100) fe anteriores + fe_funder + fe_ward"))

color_me <- which(puntuaciones$Driven.Data == 0.8197)
kable(puntuaciones) %>%
  kable_styling() %>%
  row_spec(color_me, bold = TRUE, background = "#4D934D")
```

Efectivamente, __hemos conseguido un mejor _score_ que el obtenido en clase__.

```{r}
# Guardamos el dataset
fwrite(datcompleto, file = "./data/datcompleto_clean_text.csv")
```

### 3.1.2 Lumping sobre ward y funder (mediana)

Tras la mejora en el modelo anterior, todavía quedan demasiadas categorías en _ward_ y _funder_, concretamente 2098 y 2110 categorías, respectivamente. Además, si echamos un vistazo a la proporción de aparición de las diferentes categorías de cada variable, mediante la función _prop.table_:

```{r}
prop_categorias_ward   <- summary(c(prop.table(table(datcompleto[, fe_ward])))) # fe_ward
prop_categorias_ward
```

```{r}
prop_categorias_funder <- summary(c(prop.table(table(datcompleto[, fe_funder])))) # fe_funder
prop_categorias_funder
```

Observamos que __muchas de las categorías aperecen con muy poca frecuencia__. Por ejemplo, si nos fijamos en la mediana vemos que la mitad de las categorías de _fe_ward_ aparecen con una proporción igual o inferior a 3.37e-04, así como en _fe_funder_, con una proporción aún más pequeña (del orden de 1e-5).

Por tanto, una posibilidad sería __aplicar lumping sobre aquellas categorías con una proporción de aparición muy pequeña__, agrupándose en torno a una única categoría ( _other_ ). En un primer comienzo, podemos aplicarlo sobre la mediana. Por ejemplo, en el caso de la variable _ward_, dado que la mediana se sitúa en torno a 3.367e-04, aplicamos _lumping_ __a partir de aquellas categorías con una proporción inferior a 4e-04, de forma aproximada__, haciendo uso de la función _fct_lump_prop_ del paquete _forcats_:

```{r}
#-- fe_ward
datcompleto[, fe_ward := fct_lump_prop(datcompleto[,fe_ward], 4e-04, other_level = "other")]
datcompleto$fe_ward <- as.character(datcompleto$fe_ward)

# Pasamos de 2096 a 904 categorias (menos de la mitad de categorias)
sum(length(unique(datcompleto[, fe_ward])))
```

En el caso de _funder_, __aproximamos a  2e-05__:

```{r}
#-- fe_funder
datcompleto[, fe_funder := fct_lump_prop(datcompleto[,fe_funder], 2e-05, other_level = "other")]
datcompleto$fe_funder <- as.character(datcompleto$fe_funder)

# Pasamos de 2110 a 999 categorias (menos de la mitad de categorias)
sum(length(unique(datcompleto[, fe_funder])))
```

Como podemos comprobar, __conseguimos reducir significativamente el número de categorías en ambas variables__. Una vez aplicado _lumping_ sobre dichas columnas, lanzamos nuevamente un modelo _Random Forest_:

```{r, eval=FALSE}
#-- Modelo
# Dividimos entre conjunto de entrenamiento y prueba
train <- datcompleto[c(1:fila_test-1),]
train$status_group <- vector_status_group
train$status_group <- as.factor(train$status_group)

test <- datcompleto[c(fila_test:nrow(datcompleto)),]

formula   <- as.formula("status_group~.")

# Empleamos la funcion fit_random_forest definida anteriormente
my_model_6 <- fit_random_forest(formula, train)
```

```
[1] 0.8159764
39.081 sec elapsed
```

En relación con el modelo anterior, el hecho de disminuir el número de categorías ha mejorado el resultado en el conjunto _train_ (pasando de 0.8149 a 0.8159). A continuación, analicemos el conjunto _test_:

```{r, eval=FALSE}
my_sub_6 <- make_predictions(my_model_6, dattestOr)
# guardo submission
fwrite(my_sub_6, 
    file = "./submissions/06_lumping_y_fe_sobre_funder_ward_mediana.csv")
```

```{r, echo=FALSE}
puntuaciones <- data.frame("Train accuracy" = c('-', 0.8149832, 0.8159764), 
                        "Driven Data" = c(0.8176, 0.8197, 0.8212),
                        row.names = c("Mejor accuracy en el concurso",
                                      "Num + Cat (> 1 & < 2100) fe anteriores + fe_funder + fe_ward",
                                      "Num + Cat (> 1 & < 2100) fe anteriores + lumping sobre funder + ward (mediana)"))

color_me <- which(puntuaciones$Driven.Data == 0.8212)
kable(puntuaciones) %>%
  kable_styling() %>%
  row_spec(color_me, bold = TRUE, background = "#4D934D")
```

```{r, echo=FALSE}
# Guardamos el dataset
fwrite(datcompleto, file = "./data/datcompleto_clean_text_lumping_mediana.csv")
```

__Observamos que incluso en el conjunto _test_ el _score_ ha aumentado a 0.8212__.

### 3.1.3 Lumping sobre ward y funder (primer y tercer cuartil)

En el apartado anterior, aplicamos _lumping_ sobre aquellas categorías cuya proporción de aparición se situase por debajo de la mediana, aproximadamente. No obstante, __¿Y si aplicamos el punto de corte sobre el primer o tercer cuartil?__ Es decir, si recordamos la salida del _summary_ anterior:

```{r}
prop_categorias_ward   # fe_ward
```

```{r}
prop_categorias_funder # fe_funder
```

Podríamos aplicar _lumping_ sobre el primer cuartil, de forma aproximada, o bien sobre el tercer cuartil. De este modo, __podemos comprobar si agrupando un mayor o menor número de categorías conseguimos mejorar la puntuación__ ("jugar con el punto de corte"). Para ello, recuperamos nuevamente el _dataset_ obtenido tras aplicar la función _clean_text_:

```{r}
datcompleto <- fread("./data/datcompleto_clean_text.csv")
```

#### 3.1.3.1 Sobre el primer cuartil

```{r}
#-- fe_ward (primer cuartil: 1.751e-04, aplicamos el punto de corte sobre 2e-04 aprox.)
datcompleto[, fe_ward := fct_lump_prop(datcompleto[,fe_ward], 2e-04, other_level = "other")]
datcompleto$fe_ward <- as.character(datcompleto$fe_ward)

# Pasamos de 2096 a 1483 categorias
sum(length(unique(datcompleto[, fe_ward])))
```

```{r}
#-- fe_funder (primer cuartil: 1.347e-05, aplicamos el punto de corte sobre 2e-05 aprox.)
datcompleto[, fe_funder := fct_lump_prop(datcompleto[,fe_funder], 2e-05, other_level = "other")]
datcompleto$fe_funder <- as.character(datcompleto$fe_funder)

# Pasamos de 2110 a 999 categorias (primer cuartil = mediana)
sum(length(unique(datcompleto[, fe_funder])))
```

Una vez aplicado _lumping_, recalculamos el modelo:

```{r, eval = FALSE}
#-- Modelo
# Dividimos entre conjunto de entrenamiento y prueba
train <- datcompleto[c(1:fila_test-1),]
train$status_group <- vector_status_group
train$status_group <- as.factor(train$status_group)

test <- datcompleto[c(fila_test:nrow(datcompleto)),]

formula   <- as.formula("status_group~.")
# Resultado train: 0.8159259
my_model_7 <- fit_random_forest(formula,
                                train)

my_sub_7 <- make_predictions(my_model_7, test)

fwrite(my_sub_7, file = "./submissions/07_lumping_y_fe_sobre_funder_ward_primer_cuartil.csv")
# Score: 0.8196
```

```{r, echo=FALSE}
kable(data.frame("Train accuracy" = c(0.8159764, 0.8159259), 
                        "Driven Data" = c(0.8212, 0.8196),
                        row.names = c("lumping sobre mediana",
                                      "lumping sobre primer cuartil")),
      align = 'c') %>%
  kable_styling() %>%
  row_spec(1:2, background = "white")
```

Observamos que manteniendo un mayor número de categorías, el _score_ obtenido tanto en _train_ como en _test_ empeora.

#### 3.1.3.2 Sobre el tercer cuartil

```{r}
datcompleto <- fread("./data/datcompleto_clean_text.csv")
```

```{r}
#-- fe_ward (primer cuartil: 6.330e-04, aplicamos el punto de corte sobre 7e-04 aprox.)
datcompleto[, fe_ward := fct_lump_prop(datcompleto[,fe_ward], 7e-04, other_level = "other")]
datcompleto$fe_ward <- as.character(datcompleto$fe_ward)

# Pasamos de 2096 a 452 categorias
sum(length(unique(datcompleto[, fe_ward])))
```

```{r}
#-- fe_funder (primer cuartil: 9.428-05, aplicamos el punto de corte sobre 1e-04 aprox.)
datcompleto[, fe_funder := fct_lump_prop(datcompleto[,fe_funder], 1e-04, other_level = "other")]
datcompleto$fe_funder <- as.character(datcompleto$fe_funder)

# Pasamos de 2110 a 502 categorias
sum(length(unique(datcompleto[, fe_funder])))
```

```{r, eval = FALSE}
#-- Modelo
# Dividimos entre conjunto de entrenamiento y prueba
train <- datcompleto[c(1:fila_test-1),]
train$status_group <- vector_status_group
train$status_group <- as.factor(train$status_group)

test <- datcompleto[c(fila_test:nrow(datcompleto)),]

formula   <- as.formula("status_group~.")
# Resultado train: 0.8146633
my_model_8 <- fit_random_forest(formula,
                                train)

my_sub_8 <- make_predictions(my_model_8, test)

fwrite(my_sub_8, file = "./submissions/08_lumping_y_fe_sobre_funder_ward_tercer_cuartil.csv")
# Score: 0.8203
```

```{r, echo=FALSE}
kable(data.frame("Train accuracy" = c(0.8159764, 0.8159259, 0.8146633), 
                        "Driven Data" = c(0.8212, 0.8196, 0.8203),
                        row.names = c("lumping sobre mediana",
                                      "lumping sobre primer cuartil",
                                      "lumping sobre tercer cuartil")),
      align = 'c') %>%
  kable_styling() %>%
  row_spec(1:3, background = "white")
```

Comprobamos que, pese a mejorar el _score_ del modelo al aplicar el punto de corte sobre el tercer cuartil, el resultado obtenido sobre la mediana continua siendo mejor.

Por tanto, __sobre las variables _fe_ward_ y _fe_funder_ aplicamos _lumping_ sobre la mediana de proporción de aparición de cada una de sus categorías__, reduciendo su número a 904 y 999, respectivamente.

```{r, echo=FALSE}
puntuaciones <- data.frame("Train accuracy" = c('-', 0.8149832, 0.8159764, 0.8159259, 0.8146633), 
                        "Driven Data" = c(0.8176, 0.8197, 0.8212, 0.8196, 0.8203),
                        row.names = c("Mejor accuracy en el concurso",
                                      "Num + Cat (> 1 & < 2100) fe anteriores + fe_funder + fe_ward",
                                      "Num + Cat (> 1 & < 2100) fe anteriores + lumping sobre funder + ward (mediana)",
                                      "Num + Cat (> 1 & < 2100) fe anteriores + lumping sobre funder + ward (primer cuartil)",
                                      "Num + Cat (> 1 & < 2100) fe anteriores + lumping sobre funder + ward (tercer cuartil)"))

color_me <- which(puntuaciones$Driven.Data == 0.8212)
kable(puntuaciones) %>%
  kable_styling() %>%
  row_spec(color_me, bold = TRUE, background = "#4D934D")
```

Pese a aplicar _lumping_ sobre ambas variables, continuamos teniendo demasiadas categorías: 904 en el caso de _fe_ward_ y 999 en el caso de _fe_funder_. Como consecuencia, __se plantearon diversas alternativas con el fin de reducir nuevamente el número de valores únicos sobre dichas columnas__, observando con ello si el _score_ obtenido mejora. Concretamente, se probaron cuatro técnicas mencionadas a lo largo del módulo.

1. _Hashing encoding_

2. _Frecuencias absolutas_ (realizada por el grupo ganador)

3. _Target encoding_ (realizada por el grupo ganador)

4. _Word Embedding_

De este modo, en caso de que alguna de las técnicas anteriores, sobre las variables _ward_ y _funder_, mejore el _score_ (0.8212), __se aplicará al resto de variables categóricas__. Nuevamente, recuperamos el _dataset_ sobre el que se ha obtenido el mejor _score_ (clean text + lumping mediana):

```{r}
datcompleto <- fread("./data/datcompleto_clean_text_lumping_mediana.csv")
```

### 3.1.4 Aplicando Hashing Encoding

También conocido como _Feature Hashing_, consiste en transformar un elevado conjunto de características en un vector, por medio de una función de transformación o _hash function_. No obstante, uno de los principales incovenientes de dicha técnica reside en el ratio de colisión o _collision rate_, es decir, __cuando dos categorías presentan el mismo resultado en la función hash__. Por tanto, __cuanto mayor sea el tamaño de la función _hash_, menor será el ratio de colisión__.

Para ello, haremos uso del paquete _FeatureHashing_ disponible en CRAN. Dicho paquete dispone de la función _hash.size_, el cual indica __el tamaño mínimo (teórico) que permite reducir al mínimo el ratio de colisión entre valores _hash___:

```{r}
#-- Feature Hashing (tras aplicar lumping sobre funder y ward)
# Podemos elegir el tamaño minimo (teorico) que permite reducir el ratio de colision entre valores hash
tam_minimo <- hash.size(datcompleto[, c("fe_funder", "fe_ward")])
tam_minimo
```

La función nos indica que el tamaño mínimo es de 2048. A continuación, mediante la función _hashed.model.matrix_ creamos la matriz _hash_ correspondiente con la que indexar las categorías de las columnas _ward_ y _funder_:

```{r}
mat_hash   <- hashed.model.matrix(~., datcompleto[, c("fe_funder", "fe_ward")], tam_minimo, create.mapping = TRUE)

# Comprobamos el ratio de colisión
mean(duplicated(hash.mapping(mat_hash))) # 0.3520757
```

Comprobamos que el ratio de colisión obtenido no es demasiado alto (alrededor de 0.35). Una vez elaborada la matriz, debemos __sustituir los categorías originales por su valor _hash_ correspondiente__, haciendo uso de la función _hash.mapping_, encargada de devolver la equivalencia (mapeo) entre cada categoría y su valor _hash_:

```{r}
# Sustituimos las columnas fe_funder y fe_ward por su valor hash correspondiente
vector_hash <- hash.mapping(mat_hash)
mat_hash_dt <- data.table("feature" = names(vector_hash), 
                          "values" = vector_hash)

head(mat_hash_dt)
```

Una vez obtenida la matriz de equivalencias, sustituimos en el _data.table_ original:

```{r}
# Por defecto, hashed.model.matrix añade el nombre de columna a la variable
# De forma que debemos incluirlo tambien tanto en fe_funder como en fe_ward
# para hacer coincidir las categorias
datcompleto[, fe_ward := paste0("fe_ward", fe_ward)]
datcompleto[, fe_funder := paste0("fe_funder", fe_funder)]

datcompleto[mat_hash_dt, fe_ward_hashed := i.values,  on = .(fe_ward = feature)]
datcompleto[mat_hash_dt, fe_funder_hashed := i.values,  on = .(fe_funder = feature)]
```

Además, observamos cómo el número de "categorías" (valores _hash_) se reduce a 727 y 803 valores únicos:

```{r}
#- Numero de valores unicos en fe_ward_hashed
length(unique(datcompleto$fe_ward_hashed))   # 727 categoricas
#- Numero de valores unicos en fe_funder_hashed
length(unique(datcompleto$fe_funder_hashed)) # 803 categorias
```
```{r}
# Eliminamos las columnas fe_ward y fe_funder originales
datcompleto[, `:=`(fe_funder = NULL, fe_ward = NULL)]
```

Una vez reemplazadas las categorías, entrenamos un nuevo modelo:

```{r, eval = FALSE}
my_model_9 <- fit_random_forest(formula, train)
```

```
[1] 0.8158418
35.099 sec elapsed
```

En relación al conjunto de entrenamiento, el _accuracy_ obtenido es similar al obtenido mediante _lumping_ (del orden de 0.815). A continuación, guardamos la _submission_ y comprobamos su _score_ en DrivenData:

```{r, eval=FALSE}
my_sub_9 <- make_predictions(my_model_9, test)
# guardo submission
fwrite(my_sub_9, file = "./submissions/09_lumping_fe_y_hashing_sobre_funder_ward.csv")
```

```{r, echo=FALSE}
puntuaciones <- data.frame("Train accuracy" = c('-', 0.8149832, 0.8159764, 0.8159259, 0.8146633, 0.8158418), 
                        "Driven Data" = c(0.8176, 0.8197, 0.8212, 0.8196, 0.8203, 0.8214),
                        row.names = c("Mejor accuracy en el concurso",
                                      "Num + Cat (> 1 & < 2100) fe anteriores + fe_funder + fe_ward",
                                      "Num + Cat (> 1 & < 2100) fe anteriores + lumping sobre funder + ward (mediana)",
                                      "Num + Cat (> 1 & < 2100) fe anteriores + lumping sobre funder + ward (primer cuartil)",
                                      "Num + Cat (> 1 & < 2100) fe anteriores + lumping sobre funder + ward (tercer cuartil)",
                                      "Num + Cat (> 1 & < 2100) fe anteriores + lumping (mediana) + hashed sobre funder + ward"))

color_me <- which(puntuaciones$Driven.Data == 0.8214)
kable(puntuaciones) %>%
  kable_styling() %>%
  row_spec(color_me, bold = TRUE, background = "#4D934D")
```

Sorprendentemente, __aplicando _Feature Hashing_ conseguimos mejorar el _score_ del modelo de 0.8212 a 0.8214__.

Dado que 2048 es el tamaño mínimo que permite reducir el ratio de colisión entre valores _hash_, __¿Y si reducimos dicho tamaño a la mitad, 1024? ¿Mejorará el modelo al reducir el número de categogrías únicas?__

```{r, echo=FALSE}
datcompleto <- fread("./data/datcompleto_clean_text_lumping_mediana.csv")
```

```{r}
mat_hash_2 <- hashed.model.matrix(~., datcompleto[, c("fe_funder", "fe_ward")], 2^10, create.mapping = TRUE, )
mean(duplicated(hash.mapping(mat_hash_2))) # 0.5491329
```

Lógicamente, el ratio de colisión aumenta al disminuir el tamaño (de 0.35 a 0.55, aproximadamente). Si entrenamos nuevamente el modelo con la nueva matriz _hash_:

```{r, eval=FALSE}
# Una vez realizados los mismos pasos que en el caso anterior...
my_model_9 <- fit_random_forest(formula, train)
```

```
[1] 0.8162121
35.918 sec elapsed
```

Aparentemente, el valor obtenido en el conjunto de entrenamiento mejora ligeramente (de 0.815 a 0.816). Sin embargo, al predecir el conjunto _test_, observamos que el _score_ empeora:

```{r, eval=FALSE}
my_sub_9 <- make_predictions(my_model_9, test)
# guardo submission
fwrite(my_sub_9, file = "./submissions/09_lumping_fe_y_hashing_sobre_funder_ward_2_up_10.csv")
```

```{r, echo=FALSE}
kable(data.frame("Train accuracy" = c(0.8158418, 0.8162121), 
                        "Driven Data" = c(0.8214, 0.8197),
                        row.names = c("Hashing (2048 hash size)",
                                      "Hashing (1024 hash size)")),
      align = 'c') %>%
  kable_styling() %>%
  row_spec(1:2, background = "white")
```

### 3.1.5 Aplicando Frecuencias Absolutas

Otra alternativa a _Feature Hashing_ consiste en __sustituir cada categoría en _fe_funder_ y _fe_ward_ por su frecuencia de aparición correspondiente__, un tipo de codificación con el que obtuvo buenos resultados el grupo ganador y, por ello, debemos comprobar si mejora el _score_ obtenido en _Feature Hashing_ (0.8214):

```{r, echo=FALSE}
datcompleto <- fread("./data/datcompleto_clean_text_lumping_mediana.csv")
```

```{r}
#- fe_ward
datcompleto[, fe_ward_freq := as.numeric(.N), by = fe_ward]
length(unique(datcompleto[, fe_ward_freq]))  # 129 categorias
```

```{r}
#-  fe_funder
datcompleto[, fe_funder_freq := as.numeric(.N), by = fe_funder]
length(unique(datcompleto[, fe_funder_freq])) # 169 categorias
```

```{r}
# Eliminamos las columnas originales
datcompleto[, `:=`(fe_funder = NULL, fe_ward = NULL)]
```

Observamos que el número de categorías en ambas variables se ve reducido de 904 y 999 a 129 y 169 valores únicos, respectivamente. A continuación, dividimos el conjunto de datos entre _train_ y _test_, entrenando un nuevo modelo:

```{r, eval=FALSE}
my_model_10 <- fit_random_forest(formula, train)
```

```
[1] 0.8154882
38.746 sec elapsed
```

En relación al _accuracy_ obtenido en el entrenamiento, el porcentaje es ligeramente inferior con respecto al obtenido en el apartado anterior (0.8158418). No obstante, si realizamos la predicción del conjunto _test_:

```{r, echo=FALSE}
puntuaciones <- data.frame("Train accuracy" = c('-', 0.8149832, 0.8159764, 0.8146633, 0.8159259, 0.8158418, 0.8154882), 
                        "Driven Data" = c(0.8176, 0.8197, 0.8212, 0.8203, 0.8196, 0.8214, 0.8216),
                        row.names = c("Mejor accuracy en el concurso",
                        "Num + Cat (> 1 & < 2100) fe anteriores + fe_funder + fe_ward",
                        "Num + Cat (> 1 & < 2100) fe anteriores + lumping sobre funder + ward (mediana)",
                        "Num + Cat (> 1 & < 2100) fe anteriores + lumping sobre funder + ward (tercer cuartil)",
                        "Num + Cat (> 1 & < 2100) fe anteriores + lumping sobre funder + ward (primer cuartil)",
                        "Num + Cat (> 1 & < 2100) fe anteriores + lumping (mediana) + hashed sobre funder + ward",
                        "Num + Cat (> 1 & < 2100) fe anteriores + lumping (mediana) + freq. abs. sobre funder + ward"))

color_me <- which(puntuaciones$Driven.Data == 0.8216)
kable(puntuaciones) %>%
  kable_styling() %>%
  row_spec(color_me, bold = TRUE, background = "#4D934D")
```

Comprobamos que __sustituyendo las categorías de _fe_ward_ y _fe_funder_ por sus frecuencias absolutas (en lugar de aplicar _FeatureHashing_), el _score_ obtenido aumenta de 0.8214 a 0.8216__.

### 3.1.6 Aplicando Target Encoding

#### 3.1.6.1 Empleando la media

Otra técnica propuesta para la codificación de _fe_ward_ y _fe_funder_ consiste en __reemplazar cada categoría por la agregación de la variable objetivo__ (reemplazando la variables objetivo por valores numéricos: 0, 1 y 2), sustituyendo cada categoría en _fe_ward_ y _fe_funder_ por el promedio de dicha variable. A modo de ejemplo:

```
   student grades grades_mean_by_student
1:   Marie      0                   0.75 = (0 + 0 + 1 + 2 / 4)
2:   Marie      0                   0.75
3:  Pierre      1                   1.00
4:   Louis      2                   1.75
5:   Louis      2                   1.75
6:   Marie      1                   0.75
7:   Marie      2                   0.75
8:   Louis      2                   1.75
9:   Louis      1                   1.75
```

```{r, echo=FALSE}
datcompleto <- fread("./data/datcompleto_clean_text_lumping_mediana.csv")
```

Para aplicar _target_encoding_, recuperamos únicamente las variables categóricas _fe_ward_ y _fe_funder_:

```{r}
# Recuperamos las columnas categoricas
datcat_df <- as.data.frame(datcompleto %>% select(fe_funder, fe_ward))
```

```{r, echo=FALSE}
# Convertimos las variables character a factor
datcompleto[, names(datcat_df) := lapply(.SD, as.factor), .SDcols=names(datcat_df)]
```

En muchas ocasiones, se recomienda añadir un valor de "ruido aleatorio" a las columnas del conjunto de entrenamiento por lo que separaremos nuevamente _datcompleto_ en entrenamiento y prueba:

```{r}
# Dividimos datcompleto en entrenamiento y prueba
train <- datcompleto[c(1:fila_test-1),]
train$status_group <- vector_status_group
train$status_group <- as.factor(train$status_group)

test <- datcompleto[c(fila_test:nrow(datcompleto)),]
```

A continuación, de cara la codificación __convertimos la variable objetivo en formato numérico__:

```{r}
# 0: functional ;  1: functional needs repair ; 2: non functional
train[, status_group := ifelse(status_group == "functional", 0, 
                               ifelse(status_group == "functional needs repair", 1
                                      , 2))]
```

Para realizar _Target Encoding_, empleamos la función _build_target_encoding_ disponible en el paquete _DataPreparation_, sobre la que debemos indicar el conjunto de entrenamiento, el nombre las columnas a codificar, la variable objetivo, así como la función de agregación (en este caso la media):

```{r}
#-- Target encoding (mean)
dat_encoded <- build_target_encoding(train, 
                                     cols_to_encode = names(datcat_df),
                                     target_col = "status_group",
                                     functions = "mean",
                                     verbose = TRUE)
```

Dicha función devuelve una lista con la correspondencia entre cada categoría y su valor codificado mediante _Target Encoding_. Veamos un ejemplo:

```{r}
# Creamos un data.table para observar los resultados obtenidos tras aplicar Target Encoding
dat_encoded_dt          <- as.data.table(dat_encoded)
names(dat_encoded_dt)   <- stri_replace_all_regex(names(dat_encoded_dt), 
                                                  pattern = ".*\\.",
                                                  replacement = "")

dat_encoded_dt[1, ] # Veamos un ejemplo
```

En la salida anterior, __observamos que la categoría _roman_ presenta un promedio de 0.32 con respecto a la variable objetivo__, esto es, __el estado de las bombas de agua creadas por dicha empresa, de media, es "no funcional" (0)__. Por el contrario, en la localidad de _mundindi_ las bombas de agua presentan un promedio más cercano a 1 (0.75).

Tras obtener el _Target Encoding_ de cada categoría, __debemos sustituir el valor de cada categoría por su agregación correspondiente__, tanto en el conjunto _train_ como _test_. Para ello, empleamos la función _target_encode_:

```{r}
train <- target_encode(train, target_encoding = dat_encoded)
test <- target_encode(test, target_encoding = dat_encoded)

sum(is.na(test)) # 21 valores missing en status_group_by_fe_funder
```

En el caso del conjunto _test_, detectamos valores _missing_ en la columna _status_group_by_fe_funder_. Como consecuencia, y dado que son pocos valores, se tomó la decisión de reeemplazarlos directamente por el valor medio:

```{r}
# Reemplazamos los valores nulos por la media
media <- mean(test[, status_group_mean_by_fe_funder], na.rm = TRUE)
setnafill(test, cols="status_group_mean_by_fe_funder", fill = media)
```

A continuación, __añadimos un valor de ruido al conjunto _train_, además de recodificar la variable objetivo y eliminar las columnas _fe_ward_ y _fe_funder_ originales__:

```{r}
# Añadimos ruido a los datos train
cat_cols <- paste0('status_group_mean_by_',names(datcat_df))

train[, cat_cols] <- train[, lapply(.SD, 
                                    function(x) x * rnorm(length(x), mean = 1, 
                                    sd = 0.05)), .SDcols = cat_cols]

# Recodificamos la variable objetivo
train[, status_group := ifelse(status_group == 0, "functional", 
                               ifelse(status_group == 1, "functional needs repair"
                                      , "non functional"))]

# Eliminamos las variables originales
train[, `:=`(fe_funder = NULL, fe_ward = NULL)]
test[, `:=`(fe_funder = NULL, fe_ward = NULL)]
```

Finalmente, entrenamos el modelo y realizamos las predicciones sobre el conjunto _test_:

```{r, eval=FALSE}
#-- Modelo
formula   <- as.formula("status_group~.")
# 0.8154377
my_model_11 <- fit_random_forest(formula,
                                 train)
```

```
[1] 0.8154377
39.044 sec elapsed
```

```{r, eval=FALSE}
my_sub_11 <- make_predictions(my_model_11, test)

# guardo submission
fwrite(my_sub_11, file = "./submissions/11_lumping_fe_sobre_funder_ward_target_encoding_media.csv")
```

```{r, echo=FALSE}
puntuaciones <- data.frame("Train accuracy" = c('-', 0.8149832, 0.8159764, 0.8146633, 0.8159259, 0.8158418, 0.8154882, 0.8154377), 
                        "Driven Data" = c(0.8176, 0.8197, 0.8212, 0.8203, 0.8196, 0.8214, 0.8216, 0.8164),
                        row.names = c("Mejor accuracy en el concurso",
                        "Num + Cat (> 1 & < 2100) fe anteriores + fe_funder + fe_ward",
                        "Num + Cat (> 1 & < 2100) fe anteriores + lumping sobre funder + ward (mediana)",
                        "Num + Cat (> 1 & < 2100) fe anteriores + lumping sobre funder + ward (tercer cuartil)",
                        "Num + Cat (> 1 & < 2100) fe anteriores + lumping sobre funder + ward (primer cuartil)",
                        "Num + Cat (> 1 & < 2100) fe anteriores + lumping (mediana) + hashed sobre funder + ward",
                        "Num + Cat (> 1 & < 2100) fe anteriores + lumping (mediana) + freq. abs. sobre funder + ward",
                        "Num + Cat (> 1 & < 2100) fe anteriores + lumping (mediana) + freq. abs. y target enc. (media) sobre funder + ward"))

color_me <- which(puntuaciones$Driven.Data == 0.8216)
kable(puntuaciones) %>%
  kable_styling() %>%
  row_spec(color_me, bold = TRUE, background = "#4D934D")
```

Como podemos comprobar en la tabla resultante, aplicando _Target Encoding_ ha empeorado los resultados (de 0.8216 a 0.8164).

#### 3.1.6.2 Empleando la suma

Revisando la documentación de la función _build_target_encoding_, me llamó la atención un ejemplo propuesto en el que [se emplea la función de agregación _sum_ en lugar de la media](https://rdrr.io/cran/dataPreparation/man/build_target_encoding.html). A modo de ejemplo, supongamos el siguiente _data.table_ con diferentes bombas de agua:

```
     fe_funder    fe_ward   status_group
1:       roman   mundindi              0
2:       other      natta              1
3:      unicef      other              1
4:      unicef      natta              2
5:       other   mundindi              0

0: non functional ;  1: functional needs repair ; 2: functional
```

¿Y si en lugar de aplicar la media __realizamos la suma de los pesos de cada variable objetivo__? Es decir, dado que cada categoría tiene un valor asignado (0, 1 o 2), una posibilidad sería __calcular (sumar) el total de la variable objetivo por cada categoría__, de este modo:

```
     fe_funder    fe_ward   status_group     fe_funder_sum     fe_ward_sum   
1:       roman   mundindi              0                 0       0 + 0 = 0   
2:       other      natta              1         1 + 0 = 1       1 + 2 = 3
3:      unicef      other              1         1 + 2 = 3               1
4:      unicef      natta              2         1 + 2 = 3       1 + 2 = 3
5:       other   mundindi              0         1 + 0 = 1       0 + 0 = 0

0: non functional ;  1: functional needs repair ; 2: functional
```

Podemos comprobar como empresas/entidades como _unicef_, así como regiones como es el caso de _natta_ presentan un mayor número de bombas "defectuosas", lo cual se refleja en la suma total. No obstante, un mayor valor de la suma no implica necesariamente un mayor riesgo en las bombas de agua. Un ejemplo sería el siguiente:

```
     fe_funder   status_group     fe_funder_sum   
1:       roman              2                 2 
2:      unicef              1         1 + 1 = 2
3:      unicef              1         1 + 1 = 2

0: non functional ;  1: functional needs repair ; 2: functional
```

En el caso anterior, la suma de las variables objetivo en ambos valores de _fe_funder_ es el mismo (2). Sin embargo, mientras que en el caso de _unicef_ las bombas continuan funcionando (aunque necesiten reparación), en el caso de _roman_, la única bomba que presenta no funciona. Por tanto, podemos asegurar que se trata de una representación con muy poca interpretabilidad.

No obstante, __probaremos con un nuevo modelo__, comprobando si los resultandos mejoran de cara al concurso:

```{r, eval=FALSE}
#-- Target encoding (sum)
dat_encoded <- build_target_encoding(train, 
                                      cols_to_encode = names(datcat_df),
                                      target_col = "status_group",
                                      functions = "sum", # sustituimos "mean" por "sum"
                                      verbose = TRUE)
```

```{r, echo=FALSE}
puntuaciones <- data.frame("Train accuracy" = c('-', 0.8149832, 0.8159764, 0.8146633, 0.8159259, 0.8158418, 0.8154882, 0.8154377, 0.814899), 
                        "Driven Data" = c(0.8176, 0.8197, 0.8212, 0.8203, 0.8196, 0.8214, 0.8216, 0.8164, 0.8215),
                        row.names = c("Mejor accuracy en el concurso",
                        "Num + Cat (> 1 & < 2100) fe anteriores + fe_funder + fe_ward",
                        "Num + Cat (> 1 & < 2100) fe anteriores + lumping sobre funder + ward (mediana)",
                        "Num + Cat (> 1 & < 2100) fe anteriores + lumping sobre funder + ward (tercer cuartil)",
                        "Num + Cat (> 1 & < 2100) fe anteriores + lumping sobre funder + ward (primer cuartil)",
                        "Num + Cat (> 1 & < 2100) fe anteriores + lumping (mediana) + hashed sobre funder + ward",
                        "Num + Cat (> 1 & < 2100) fe anteriores + lumping (mediana) + freq. abs. sobre funder + ward",
                        "Num + Cat (> 1 & < 2100) fe anteriores + lumping (mediana) + freq. abs. y target enc. (media) sobre funder + ward",
                        "Num + Cat (> 1 & < 2100) fe anteriores + lumping (mediana) + freq. abs. y target enc. (suma) sobre funder + ward"))

color_me <- which(puntuaciones$Driven.Data == 0.8216)
kable(puntuaciones) %>%
  kable_styling() %>%
  row_spec(color_me, bold = TRUE, background = "#4D934D")
```

Curiosamente, el resultado obtenido __mejora con respecto al cálculo mediante la media, aunque sigue siendo mejor (por poca diferencia), la codificación por frecuencias absolutas__.

### 3.1.7 Aplicando Word Embedding

Sin duda, uno de los grandes avances en el campo del procesamiento del lenguaje han sido las técnicas de _Word Embedding_, que permiten __la representación vectorial (n-dimensional) de una palabra o _token___. Por ello, de cara a la práctica se planteó la posibilidad de __realizar una representación vectorial de las diferentes categorías en las columnas _fe_ward_ y _fe_funder___, con el objetivo de observar si la precisión del modelo mejora o no.

Para ello, haremos uso de dos paquetes, pertenecientes al conjunto de librerías _tidymodels_:

1. [_embed_](https://embed.tidymodels.org): permite la creación de modelos _word-embedding_, mediante el uso de la librería _TensorFlow_.

2. [_recipes_](https://recipes.tidymodels.org): se emplea fundamentalmente para el preprocesamiento de datos, mediante el uso de "recetas" definidas de forma secuencial por medios de _pipelines_.

En primer lugar, y una vez aplicado _lumping_ sobre _ward_ y _funder_, __dividimos nuevamente los datos en entrenamiento y test__:

```{r, echo=FALSE}
datcompleto <- fread("./data/datcompleto_clean_text_lumping_mediana.csv")
```

```{r}
# 1. Comprobamos que las variables categoricas estan codificadas como factor
emb_cols <- c("fe_ward", "fe_funder")
datcompleto[,(emb_cols):= lapply(.SD, as.factor), .SDcols = emb_cols]

# Dividimos en train y test
train <- datcompleto[c(1:fila_test-1),]
train$status_group <- vector_status_group
train$status_group <- as.factor(train$status_group)

test <- datcompleto[c(fila_test:nrow(datcompleto)),]
```

Una vez dividido el conjunto de datos, elaboramos el modelo _embedded_. Para ello, mediante la función _recipe_ elaboramos una "receta" para la transformación de variables, concretamente sobre _fe_ward_, _fe_funder_ y _status_group_. Sobre dicha receta o fórmula aplicamos la función _step_embed_ del paquete _embed_, con la que realizar la transformación _Word Embedding_. Por defecto, el número de dimensiones es de 2, por lo que probaremos inicialmente con dicho valor:

```{r}
# 2. Elaboramos el modelo embedded (empezando con dimensionalidad 2)
emb_cols_target <- c(emb_cols, "status_group")

base_recipe <- recipe(status_group ~ ., train[, ..emb_cols_target])
for(feat in emb_cols){
  base_recipe <- base_recipe %>% 
    step_embed({{feat}},
                num_terms = 2,
                outcome = vars(status_group),
                options = embed_control(epochs = 5, validation_split = 0.2)
    )
}
```

Finalmente, una vez elaborada la "receta" para la transformación de ambas columnas, __lo aplicamos inicialmente sobre el conjunto _train_ y posteriormente sobre el conjunto _test___, haciendo uso de las funciones _prep_ y _bake_, respectivamente:

```{r}
# 3. Creacion de las columnas Word Embeddings (train y test)
#    Sobre el conjunto train, elaboramos la "receta": word_embeddings
train_prepped <- prep(base_recipe, train[, ..emb_cols_target])
#    Sobre el conjunto test, lo aplicamos
test_prepped <-  bake(train_prepped, test[, ..emb_cols])

train_final  <-  cbind(as.data.table(juice(train_prepped)),
                      train[, setdiff(names(train), emb_cols_target), with = FALSE]
                 )

test_final   <-  cbind(test_prepped,
                     test[, setdiff(names(test), emb_cols), with = FALSE]
                 )
# Echemos un vistazo a las columnas transformadas
train_final[1, c("fe_ward_embed_1", "fe_ward_embed_2", "fe_funder_embed_1", "fe_funder_embed_2")]
```

Como podemos comprobar, cada categoría se transforma en dos columnas con su representación bi-dimensional correspondiente ¿Mejorará el modelo si aplicamos Word Embedding sobre _fe_ward_ y _fe_funder_? 

```{r, eval=FALSE}
# Train
my_model_12 <- fit_random_forest(formula, train_final)

# Predicciones
my_sub_12  <- make_predictions(my_model_12, test_final)
```

```{r, echo=FALSE}
puntuaciones <- data.frame("Train accuracy" = c('-', 0.8149832, 0.8159764, 0.8146633, 0.8159259, 
                                             0.8158418, 0.8154882, 0.8154377, 0.814899, 0.8155724), 
                        "Driven Data" = c(0.8176, 0.8197, 0.8212, 0.8203, 0.8196, 0.8214, 0.8216, 0.8164, 0.8215,
                                              0.8204),
                        row.names = c("Mejor accuracy en el concurso",
                        "Num + Cat (> 1 & < 2100) fe anteriores + fe_funder + fe_ward",
                        "Num + Cat (> 1 & < 2100) fe anteriores + lumping sobre funder + ward (mediana)",
                        "Num + Cat (> 1 & < 2100) fe anteriores + lumping sobre funder + ward (tercer cuartil)",
                        "Num + Cat (> 1 & < 2100) fe anteriores + lumping sobre funder + ward (primer cuartil)",
                        "Num + Cat (> 1 & < 2100) fe anteriores + lumping (mediana) + hashed sobre funder + ward",
                        "Num + Cat (> 1 & < 2100) fe anteriores + lumping (mediana) + freq. abs. sobre funder + ward",
                        "Num + Cat (> 1 & < 2100) fe anteriores + lumping (mediana) + freq. abs. y target enc. (media) sobre funder + ward",
                        "Num + Cat (> 1 & < 2100) fe anteriores + lumping (mediana) + freq. abs. y target enc. (suma) sobre funder + ward",
                        "Num + Cat (> 1 & < 2100) fe anteriores + lumping (mediana) + word embed sobre funder y ward (dim = 2)"))

color_me <- which(puntuaciones$Driven.Data == 0.8216)
kable(puntuaciones) %>%
  kable_styling() %>%
  row_spec(color_me, bold = TRUE, background = "#4D934D")
```

Creando un modelo _Word Embedding_ sobre ambas variables, el resultado no parece mejorar __¿Y si aumentamos el número de dimensiones en la representación vectorial?__ Normalmente, los modelos _Word Embedding_ suelen emplear decenas e incluso cientos de dimensiones en su representación.

```{r, echo=FALSE}
kable(data.frame("Train accuracy" = c(0.8155724, 0.8150337, 0.8145286), 
                        "Driven Data" = c(0.8204, 0.8192, 0.8157),
                        row.names = c("Word Embedding (Dim = 2)",
                                      "Word Embedding (Dim = 5)",
                                      "Word Embedding (Dim = 10)")),
      align = 'c') %>%
  kable_styling() %>%
  row_spec(1:3, background = "white")
```

Pese a aumentar el número de dimensiones (y con ello la complejidad), la precisión del modelo disminuye.

__Conclusión__: dado el mejor _score_ obtenido, __elegimos como mejor alternativa la transformación por frecuencias absolutas, con respecto a las variables categóricas__.

### 3.1.8 Transformar por frecuencias absolutas el resto de variables categóricas

A la vista de las transformaciones anteriores, la que mejor resultado ha obtenido ha sido mediante frecuencias absolutas. No obstante, y dado que solo las hemos aplicado sobre _fe_ward_ y _fe_funder_ ¿Y si la aplicamos sobre el resto de variables categóricas, concretamente de tipo carácter?

Para ello, y una vez aplicado _lumping_ sobre _ward_ y _funder_, __realizamos la imputación de las variables categóricas (character) por sus correspondientes frecuencias absolutas__. Inicialmente, almacenamos en una variable los nombres de las columnas a imputar:

```{r, echo=FALSE}
datcompleto <- fread("./data/datcompleto_clean_text_lumping_mediana.csv")
```

```{r}
#-- Imputacion de las variables categoricas por sus frecuencias absolutas
cat_cols <- names(datcompleto[, which(sapply(datcompleto, is.character)), with = FALSE])
```

Antes de realizar la imputación, guardamos en una variable el número de valores únicos de cada columna, con el fin de contrastar posteriormente la variación en el número de categorías:

```{r}
#   Antes de imputar
freq_antes_fe <- apply(datcompleto[, ..cat_cols], 2, function(x) length(unique(x)))
```

A continuación, __sustituimos el valor de cada columna por su correspondiente frecuencia absoluta, además de eliminar las columnas originales__:

```{r}
# Sustitucion de cada valor por su frecuencia absoluta
for (cat_col in cat_cols) {
  datcompleto[, paste0("fe_", cat_col) := as.numeric(.N), by = cat_col]
}
# fe_funder y fe_ward aparecen como fe_fe_ward y fe_fe_funder
# por lo que lo reemplazamos por fe_ward y fe_funder
names(datcompleto) <- stri_replace_all_fixed(names(datcompleto),
                                             "fe_fe_", "fe_")

# Eliminamos las columnas originales
for (cat_col in cat_cols) {
  datcompleto[, paste(cat_col) := NULL]
}
```

Una vez transformadas las columnas, creamos una nueva variable con el número de valores únicos de cada columna:

```{r}
new_cat_cols <- paste0("fe_", stri_replace_all_fixed(cat_cols, "fe_", ""))

freq_despues_fe <- apply(datcompleto[, ..new_cat_cols], 2, function(x) length(unique(x)))

#-- Solo cambian funder, ward y lga en relacion al numero de categorias
data.table(rbind(freq_antes_fe, freq_despues_fe), keep.rownames = TRUE)
```

Analizando la tabla resultante, __podemos comprobar como tan solo _funder_, _ward_ y _lga_ se ven reducidos en cuanto al número de valores únicos se refiere__. No obstante, veamos los resultados obtenidos tras entrenar el nuevo modelo:

```{r, eval=FALSE}
my_model_13 <- fit_random_forest(formula, train)

my_sub_13   <- make_predictions(my_model_13, test)
```

```{r, echo=FALSE}
puntuaciones <- data.frame("Train accuracy" = c('-', 0.8149832, 0.8159764, 0.8146633, 0.8159259, 
                                             0.8158418, 0.8154882, 0.8154377, 0.814899, 0.8155724,
                                             0.8157071), 
                        "Driven Data" = c(0.8176, 0.8197, 0.8212, 0.8203, 0.8196, 0.8214, 0.8216, 0.8164, 0.8215,
                                              0.8204, 0.8226),
  row.names = c("Mejor accuracy en el concurso",
  "Num + Cat (> 1 & < 2100) fe anteriores + fe_funder + fe_ward",
  "Num + Cat (> 1 & < 2100) fe anteriores + lumping sobre funder + ward (mediana)",
  "Num + Cat (> 1 & < 2100) fe anteriores + lumping sobre funder + ward (tercer cuartil)",
  "Num + Cat (> 1 & < 2100) fe anteriores + lumping sobre funder + ward (primer cuartil)",
  "Num + Cat (> 1 & < 2100) fe anteriores + lumping (mediana) + hashed sobre funder + ward",
  "Num + Cat (> 1 & < 2100) fe anteriores + lumping (mediana) + freq. abs. sobre funder + ward",
  "Num + Cat (> 1 & < 2100) fe anteriores + lumping (mediana) + freq. abs. y target enc. (media) sobre funder + ward",
  "Num + Cat (> 1 & < 2100) fe anteriores + lumping (mediana) + freq. abs. y target enc. (suma) sobre funder + ward",
  "Num + Cat (> 1 & < 2100) fe anteriores + lumping (mediana) + word embed sobre funder y ward (dim = 2)",
  "Num + Cat (> 1 & < 2100) fe anteriores + lumping (mediana) sobre funder y ward + freq. abs. cat."))

color_me <- which(puntuaciones$Driven.Data == 0.8226)
kable(puntuaciones) %>%
  kable_styling() %>%
  row_spec(color_me, bold = TRUE, background = "#4D934D")
```

Codificando las variables categóricas por sus frecuencias absolutas __mejoramos los resultados obtenidos en el conjunto _test___, pasando de 0.8216 a 0.8226.

### 3.1.9 Variables scheme_name e installer

Hasta ahora, hemos trabajado con variables categóricas que tuviesen menos de 2100 valores únicos. Sin embargo, existen variables con un mayor número de categorías como _scheme_name_, _installer_, _subvillage_ y _wpt_name_. Con el propósito de seguir un orden, __empezamos añadiendo a nuestro dataset aquellas variables con menos de 10000 categorías, esto es, _scheme_name_ e _installer___.

Sobre dichos campos aplicaremos exactamente los mismos pasos que los realizados hasta ahora con las variables categóricas (y con los que se han obtenido mejores resultados):

1. Limpieza de _strings_ (conversión a minúsculas + eliminación de espacios en blanco + eliminación de signos de puntuación)

2. Aplicar _lumping_ sobre la mediana en la proporción de apariciones

3. Reemplazar las categorías por sus frecuencias absolutas

```{r}
# Incluimos las variables scheme_name e installer en datcompleto
datcompleto[, scheme_name := datcat_mas_1000_mas_logicas_dr[, scheme_name]]
datcompleto[, installer   := datcat_mas_1000_mas_logicas_dr[, installer]]
```

```{r}
#-- fe_scheme_name
#   Numero de categorías antes de aplicar clean_text
length(unique(datcompleto[, scheme_name]))
```

```{r}
#-- fe_installer
#   Numero de categorías antes de aplicar clean_text
length(unique(datcompleto[, installer]))
```

```{r}
cols <- c('installer', 'scheme_name')
datcompleto[ , paste0('fe_',cols) := lapply(.SD, clean_text), .SDcols = cols]
rm(cols)

#   Numero de categorías tras aplicar clean_text
length(unique(datcompleto[, fe_scheme_name]))
```

```{r}
length(unique(datcompleto[, fe_installer]))
```

Como podemos observar en las salidas anteriores, el número de categorías se ha visto reducido en ambas variables. A continuación, aplicamos _lumping_ sobre dichas columnas:

```{r}
#-- fe_scheme_name
summary(c(prop.table(table(datcompleto[, fe_scheme_name]))))
```

```{r}
#-- Aplicamos lumping sobre la mediana (50 % de categorias con una proporcion menor a 8.1e-05, aprox.)
datcompleto[, fe_scheme_name := fct_lump_prop(datcompleto[,fe_scheme_name], 8.1e-05, other_level = "other")]
datcompleto$fe_scheme_name <- as.character(datcompleto$fe_scheme_name)

datcompleto[, scheme_name := NULL]
# Pasamos de 2615 a 1205
length(unique(datcompleto[, fe_scheme_name]))
```

```{r}
#-- fe_installer
summary(c(prop.table(table(datcompleto[, fe_installer]))))
```
```{r}
#-- Aplicamos lumping sobre la mediana (50 % de categorias con una proporcion menor a 2e-05, aprox.)
datcompleto[, fe_installer := fct_lump_prop(datcompleto[,fe_installer], 2e-05, other_level = "other")]
datcompleto$fe_installer <- as.character(datcompleto$fe_installer)

datcompleto[, installer := NULL]
# Pasamos de 2069 a 1029 categorias
length(unique(datcompleto[, fe_installer]))
```

Finalmente, una vez reducido el número de categorías al 50 %, aproximadamente, __realizamos la transformación por frecuencias absolutas__:

```{r}
#-- Creamos una funcion para evitar redundancia de codigo
impute_freq_abs <- function(cat_cols) {

  #   Antes de imputar
  freq_antes_fe <- apply(datcompleto[, ..cat_cols], 2, function(x) length(unique(x)))
  
  for (cat_col in cat_cols) {
    datcompleto[, paste0("fe_", cat_col) := as.numeric(.N), by = cat_col]
  }
  names(datcompleto) <- stri_replace_all_fixed(names(datcompleto), "fe_fe_", "fe_")
  
  for (cat_col in cat_cols) {
    datcompleto[, paste(cat_col) := NULL]
  }
  new_cat_cols <- paste0("fe_", stri_replace_all_fixed(cat_cols, "fe_", ""))
  
  #  Despues de imputar
  freq_despues_fe <- apply(datcompleto[, ..new_cat_cols], 2, function(x) length(unique(x)))
  
  comparacion_freq_dt <- data.table(rbind(freq_antes_fe[cat_cols], freq_despues_fe[new_cat_cols]))
  
  print(comparacion_freq_dt)
  return(datcompleto)
}
```
```{r}
# Imputacion de las variables categoricas por sus frecuencias absolutas
cat_cols <- names(datcompleto[, which(sapply(datcompleto, is.character)), with = FALSE])

datcompleto <- impute_freq_abs(cat_cols)
```

Comprobamos como el número de categorías en _scheme_name_ e _installer_ ha disminuido a 126 y 164 valores únicos, respectivamente. 

Una vez depuradas ambas columnas, lanzamos el nuevo modelo:

```{r, eval=FALSE}
my_model_14 <- fit_random_forest(formula, train)

my_sub_14 <- make_predictions(my_model_15, test)
```

```{r, echo=FALSE}
kable(data.frame("Train accuracy" = c(0.8158081), 
                        "Driven Data" = c(0.8231),
                        row.names = c("Num + Cat (> 1 & < 10000) fe anteriores + lumping (mediana) + freq. abs. cat.")),
      align = 'c') %>%
  kable_styling() %>%
  row_spec(1, background = "white")
```

Aparentemente, incluir ambas variables obtiene un mejor resultado (de 0.8226 a 0.8231). No obstante, __¿Merece la pena incluir ambas variables?__ Es decir, ¿Mejoraría el modelo si descartamos una de ellas? Veamos:

```{r, echo=FALSE}
kable(data.frame("Train accuracy" = c(0.8158081, 0.8160774, 0.8161616), 
                        "Driven Data" = c(0.8231, 0.8211, 0.8239),
                        row.names = c("Empleando tanto scheme_name como installer",
                                      "Empleando unicamente installer",
                                      "Empleando unicamente scheme_name")),
             align = 'c') %>%
  kable_styling() %>%
  row_spec(3, background = "#4D934D")
```

Curiosamente, si descartamos la variable _installer_, el _score_ del modelo mejora de 0.8231 a 0.8239. Por tanto, en lugar en emplear ambas columnas __añadiremos únicamente _scheme_name___:

```{r, echo=FALSE}
puntuaciones <- data.frame("Train accuracy" = c('-', 0.8149832, 0.8159764, 0.8146633, 0.8159259, 
                                             0.8158418, 0.8154882, 0.8154377, 0.814899, 0.8155724,
                                             0.8157071, 0.8161616), 
                        "Driven Data" = c(0.8176, 0.8197, 0.8212, 0.8203, 0.8196, 0.8214, 0.8216, 0.8164, 0.8215,
                                              0.8204, 0.8226, 0.8239),
  row.names = c("Mejor accuracy en el concurso",
  "Num + Cat (> 1 & < 2100) fe anteriores + fe_funder + fe_ward",
  "Num + Cat (> 1 & < 2100) fe anteriores + lumping sobre funder + ward (mediana)",
  "Num + Cat (> 1 & < 2100) fe anteriores + lumping sobre funder + ward (tercer cuartil)",
  "Num + Cat (> 1 & < 2100) fe anteriores + lumping sobre funder + ward (primer cuartil)",
  "Num + Cat (> 1 & < 2100) fe anteriores + lumping (mediana) + hashed sobre funder + ward",
  "Num + Cat (> 1 & < 2100) fe anteriores + lumping (mediana) + freq. abs. sobre funder + ward",
  "Num + Cat (> 1 & < 2100) fe anteriores + lumping (mediana) + freq. abs. y target enc. (media) sobre funder + ward",
  "Num + Cat (> 1 & < 2100) fe anteriores + lumping (mediana) + freq. abs. y target enc. (suma) sobre funder + ward",
  "Num + Cat (> 1 & < 2100) fe anteriores + lumping (mediana) + word embed sobre funder y ward (dim = 2)",
  "Num + Cat (> 1 & < 2100) fe anteriores + lumping (mediana) sobre funder y ward + freq. abs. cat.",
  "Num + Cat (> 1 & < 10000) fe anteriores + lumping (mediana) + freq. abs. cat. (sin installer)"))

color_me <- which(puntuaciones$Driven.Data == 0.8239)
kable(puntuaciones) %>%
  kable_styling() %>%
  row_spec(color_me, bold = TRUE, background = "#4D934D")
```

```{r}
# Guardamos datcompleto
datcompleto[, fe_installer := NULL]
fwrite(datcompleto, 
       file = "./data/datcompleto_clean_text_lumping_mediana_freq_abs_cat_menos_installer.csv")
```

### 3.1.10 Añadiendo subvillage

Hasta el momento, hemos empleado variables con menos de 10.000 valores únicos __¿Y si damos nuevamente un salto y añadimos la variable _subvillage_?__ Por supuesto, realizando los mismos pasos que en el resto de variables categóricas (limpieza de textos, _lumping_ y recategorización a frecuencias absolutas):

```{r}
# Incluimos la variable subvillage en dat_completo
datcompleto[, subvillage := datcat_mas_1000_mas_logicas_dr[, subvillage]]
```

```{r}
#-- fe_subvillage
#   Numero de categorías antes de aplicar clean_text
length(unique(datcompleto[, subvillage]))
```

```{r}
cols <- c('subvillage')
datcompleto[ , paste0('fe_',cols) := lapply(.SD, clean_text), .SDcols = cols]
rm(cols)

#   Numero de categorías tras aplicar clean_text
length(unique(datcompleto[, fe_subvillage]))
```

Nuevamente, el número de valores únicos se ve reducido de 21.426 a 21.295. A continuación, aplicamos _lumping_ e imputación por frecuencias absolutas:

```{r}
#-- fe_subvillage
summary(c(prop.table(table(datcompleto[, fe_subvillage]))))
```

```{r}
#-- Aplicamos lumping sobre la mediana (50 % de categorias con una proporcion menor a 3e-05, aprox.)
datcompleto[, fe_subvillage := fct_lump_prop(datcompleto[,fe_subvillage], 3e-05, other_level = "other")]
datcompleto$fe_subvillage <- as.character(datcompleto$fe_subvillage)

datcompleto[, subvillage := NULL]
# Pasamos de 21295 a 7511
length(unique(datcompleto[, fe_subvillage]))
```

```{r}
#-- Finalmente, realizamos la imputacion por frecuencias absolutas
cat_cols <- names(datcompleto[, which(sapply(datcompleto, is.character)), with = FALSE])

datcompleto <- impute_freq_abs(cat_cols)
```

Como primera impresión, __el número de categorías en la variable _subvillage_ se ha visto reducido drásticamente (hasta 109 valores únicos)__, de forma que podemos estar perdiendo información relevante que permita ganar en puntuación, aunque sea en el orden de milésimas. No obstante, probamos a entrenar el nuevo modelo y realizar la _submission_ correspondiente:

```{r, eval=FALSE}
# Train
my_model_14 <- fit_random_forest(formula, train)

# Submission
my_sub_14   <- make_predictions(my_model_14, test)
```

```{r, echo=FALSE}
kable(data.frame("Train accuracy" = c(0.8161616, 0.8165993), 
                        "Driven Data" = c(0.8239, 0.8219),
                        row.names = c("Mejor accuracy hasta el momento",
                                      "Num + Cat (> 1 & < 40000) fe anteriores + lumping (mediana) + freq. abs. cat. (sin installer)")),
      align = 'c') %>%
  kable_styling() %>%
  row_spec(1:2, background = "white")
```

Como podemos comprobar en la tabla anterior, el hecho de añadir la variable _subvillage_ ha provocado que el modelo empeore, pasando de 0.8239 a 0.8219. Esto puede ser debido al hecho de haber reducido de forma significativa el número de valores únicos en _subvillage_, por lo que veamos cómo quedaría el modelo __sin aplicar ninguna técnica de depuración sobre dicha columna__:

```{r, echo=FALSE}
kable(data.frame("Train accuracy" = c(0.8161616, 0.8165993, 0.8162963), 
                        "Driven Data" = c(0.8239, 0.8219, 0.8220),
                        row.names = c("Mejor accuracy hasta el momento",
                                      "Num + Cat (> 1 & < 40000) fe anteriores + lumping (mediana) + freq. abs. cat. (sin installer)",
                                      "Sin depurar subvillage (sin installer)")),
      align = 'c') %>%
  kable_styling() %>%
  row_spec(1:3, background = "white")
```

Incluso sin depurar la variable, el _score_ obtenido continua siendo menor al mejor modelo obtenido,  __por lo que descartamos añadir dicha columna__.

### 3.1.11 Añadiendo wpt_name

Junto con _subvillage_, nos queda por añadir una variable con un elevado número de categorías: _wpt_name_, con 45.684 valores únicos. Del mismo modo que en el apartado anterior, aplicamos el mismo proceso de depuración sobre dicha variable.

```{r}
# Eliminamos fe_subvillage e incluimos la variable wpt_name en dat_completo
datcompleto[, subvillage := NULL]
datcompleto[, wpt_name   := datcat_mas_1000_mas_logicas_dr[, wpt_name]]
```

```{r}
#-- fe_wpt_name
#   Numero de categorías antes de aplicar clean_text
length(unique(datcompleto[, wpt_name]))
```

```{r}
cols <- c('wpt_name')
datcompleto[ , paste0('fe_',cols) := lapply(.SD, clean_text), .SDcols = cols]
rm(cols)

#   Numero de categorías tras aplicar clean_text
length(unique(datcompleto[, fe_wpt_name]))
```

Nuevamente, el número de categorías se reduce tras aplicar la función _clean_text_. Por último, si aplicamos _lumping_ e imputamos por frecuencias absolutas:

```{r}
#-- Aplicamos lumping sobre la mediana (50 % de categorias con una proporcion menor a 2e-05, aprox.)
datcompleto[, fe_wpt_name := fct_lump_prop(datcompleto[,fe_wpt_name], 2e-05, other_level = "other")]
datcompleto$fe_wpt_name <- as.character(datcompleto$fe_wpt_name)

datcompleto[, wpt_name := NULL]
# Pasamos de 45042 a 5924
length(unique(datcompleto[, fe_wpt_name]))
```

```{r}
#-- Finalmente, realizamos la imputacion por frecuencias absolutas
cat_cols <- names(datcompleto[, which(sapply(datcompleto, is.character)), with = FALSE])

datcompleto <- impute_freq_abs(cat_cols)
```

__Cabe destacar la considerable disminución en el número de categorías__, por lo que es posible que estemos perdiendo información relevante de cara al modelo, del mismo modo que sucedía con _subvillage_. No obstante, si entrenamos el nuevo modelo:

```{r, eval=FALSE}
# Train
my_model_15 <- fit_random_forest(formula, train)

# Submission
my_sub_15   <- make_predictions(my_model_18, test)
```

```{r, echo=FALSE}
kable(data.frame("Train accuracy" = c(0.8161616, 0.8165993), 
                        "Driven Data" = c(0.8239, 0.8236),
                        row.names = c("Mejor accuracy hasta el momento",
                                      "Num + Cat (> 1 & < 60000) fe anteriores + lumping (mediana) + freq. abs. cat. (sin installer y subvillage)")),
      align = 'c') %>%
  kable_styling() %>%
  row_spec(1:2, background = "white")
```

Sorprendentemente, pese al haber reducido drásticamente el número de categorías, el _score_ obtenido no se aleja tanto del mejor modelo hasta el momento (0.8236 frente a 0.8239) __¿Y si mantenemos la variable _wpt_name_ original?__ Es decir, sin depurar:

```{r, echo=FALSE}
kable(data.frame("Train accuracy" = c(0.8161616, 0.8165993, 0.8155892), 
                        "Driven Data" = c(0.8239, 0.8236, 0.8220),
                        row.names = c("Mejor accuracy hasta el momento",
                                      "Num + Cat (> 1 & < 60000) fe anteriores + lumping (mediana) + freq. abs. cat. (sin installer y subvillage)",
                                      "Sin depurar wpt_name")),
      align = 'c') %>%
  kable_styling() %>%
  row_spec(1:3, background = "white")
```

Por el contrario, si mantenemos la variable original, el hecho de disponer de un elevado número de categorías provoca una disminución en el _score_ del modelo.

Por tanto, de las variables con un elevado número de categorías __nos quedaremos únicamente con _ward_, _funder_ y _scheme_name___:

```{r, echo=FALSE}
kable(data.frame("Train accuracy" = c(0.8161616), 
                        "Driven Data" = c(0.8239),
                        row.names = c("Num + Cat (> 1 & < 10000) fe anteriores + lumping (mediana) + freq. abs. cat. (sin installer)")),
             align = 'c') %>%
  kable_styling() %>%
  row_spec(1, background = "white")
```

### 3.1.12 Añadiendo variables lógicas

Como últimas variables, nos quedan por añadir _permit_ y _public meeting_, ambas variables lógicas. Sin embargo, nos encontramos un problema: __ambas columnas presentan valores _missing___, tanto en el conjunto _train_:

```{r, echo=FALSE}
datcompleto <- fread("./data/datcompleto_clean_text_lumping_mediana_freq_abs_cat_menos_installer.csv")

# Incluimos las variables permit y public_meeting en datcompleto
datcompleto[, permit           := datcat_mas_1000_mas_logicas_dr[, permit]]
datcompleto[, public_meeting   := datcat_mas_1000_mas_logicas_dr[, public_meeting]]

dattestOr   <- fread("./data/dattestOr.csv", data.table = TRUE)
```

```{r}
# Numero NAs permit y public_meeting
sum(is.na(datcompleto[, permit]))
sum(is.na(datcompleto[, public_meeting]))
```

Como en el conjunto _test_:

```{r}
# Numero NAs permit y public_meeting
sum(is.na(dattestOr[, permit]))
sum(is.na(dattestOr[, public_meeting]))
```

Por tanto, dado que existen valores _missing_, debemos imputarlos por medio de la función _missRanger_ (empleando los mismos parámetros que los utilizados durante el concurso, es decir, k = 5 y número de iteraciones = 100).

```{r}
# Imputacion de permit y public_meeting por missRanger
datcompleto_imp <- missRanger(datcompleto,
                            pmm.k = 5,
                            seed = 1234,
                            maxiter = 100, 
                            verbose = 0)
```
```{r}
# Comprobamos que no existen valores missing
sum(is.na(datcompleto_imp))
```

A continuación, podría resultar de gran utilidad __incluir dos variables adicionales ( _is_na_public_meeting_ e _is_na_permit_ ), que indiquen si el valor original, previamente de ser imputado, era un valor _missing_ o no (1,0)__:

```{r}
# Creamos dos columnas adicionales que indiquen si el valor original era o no NA
datcompleto_imp[, is_na_public_meeting := ifelse(is.na(datcompleto[, public_meeting]), 1, 0)]
datcompleto_imp[, is_na_permit := ifelse(is.na(datcompleto[, permit]), 1, 0)]
```

Finalmente, lanzamos el nuevo modelo:

```{r, eval=FALSE}
# Train
my_model_15 <- fit_random_forest(formula, train)

# Submission
my_sub_15   <- make_predictions(my_model_15, test)
```

```{r, echo=FALSE}
kable(data.frame("Train accuracy" = c(0.8161616, 0.8168855), 
                        "Driven Data" = c(0.8239, 0.8251),
                        row.names = c("Mejor accuracy hasta el momento",
                                      "Num + Cat (> 1 & < 10000) fe anteriores + lumping (mediana) + freq. abs. cat. (sin installer) + variables logicas")),
      align = 'c') %>%
  kable_styling() %>%
  row_spec(1:2, background = "white")
```

Añadiendo ambas variables lógicas, el modelo ha mejorado de 0.8239 a 0.8251, __superando incluso a la mejor puntuación obtenida en clase__ (0.8248).

No obstante, si hacemos "zoom" sobre el gráfico de importancia:

```{r, echo=FALSE, fig.align='center', fig.height=12, fig.width=12}
include_graphics('./charts/19_lumping_fe_freq_abs_sobre_funder_ward_scheme_name_resto_categoricas_y_permit_public_meeting.png')
```

Observamos que las variables binarias _is_na_ apenas tienen importancia en el modelo ¿Y si las eliminamos?

```{r, echo=FALSE}
kable(data.frame("Train accuracy" = c(0.8161616, 0.8168855, 0.8171886), 
                        "Driven Data" = c(0.8239, 0.8251, 0.8235),
                        row.names = c("Mejor accuracy hasta el momento",
                                      "Num + Cat (> 1 & < 10000) fe anteriores + lumping (mediana) + freq. abs. cat. (sin installer) + variables logicas",
                                      "Eliminando is_na_public_meeting e is_na_permit")),
      align = 'c') %>%
  kable_styling() %>%
  row_spec(1:3, background = "white")
```

Pese a la poca importancia que presentaban, eliminando ambas columnas ha provocado que el modelo empeore en cuanto a _score_ se refiere. Por otro lado, tanto _public_meeting_ como _permit_ son muy similares en cuanto a importancia ¿Mejoraría el modelo si descartamos una de ellas? 

```{r, echo=FALSE}
kable(data.frame("Train accuracy" = c(0.8161616, 0.8168855, 0.8171886, 0.8168519, 0.8172559), 
                        "Driven Data" = c(0.8239, 0.8251, 0.8235, 0.8234, 0.8236),
                        row.names = c("Mejor accuracy hasta el momento",
                                      "Num + Cat (> 1 & < 10000) fe anteriores + lumping (mediana) + freq. abs. cat. (sin installer) + variables logicas",
                                      "Eliminando is_na_public_meeting e is_na_permit",
                                      "Empleando solo permit (mas su columna is_na)",
                                      "Empleando solo public_meeting (mas su columna is_na)")),
      align = 'c') %>%
  kable_styling() %>%
  row_spec(1:5, background = "white")
```

Nuevamente, en ninguno de los casos el modelo se ve mejorado. __Por tanto, mantenemos ambas variables lógicas más sus correspondientes columnas _is_na___.

```{r, echo=FALSE}
puntuaciones <- data.frame("Train accuracy" = c('-', 0.8149832, 0.8159764, 0.8146633, 0.8159259, 
                                             0.8158418, 0.8154882, 0.8154377, 0.814899, 0.8155724,
                                             0.8157071, 0.8161616, 0.8168855), 
                        "Driven Data" = c(0.8176, 0.8197, 0.8212, 0.8203, 0.8196, 0.8214, 0.8216, 0.8164, 0.8215,
                                              0.8204, 0.8226, 0.8239, 0.8251),
  row.names = c("Mejor accuracy en el concurso",
  "Num + Cat (> 1 & < 2100) fe anteriores + fe_funder + fe_ward",
  "Num + Cat (> 1 & < 2100) fe anteriores + lumping sobre funder + ward (mediana)",
  "Num + Cat (> 1 & < 2100) fe anteriores + lumping sobre funder + ward (tercer cuartil)",
  "Num + Cat (> 1 & < 2100) fe anteriores + lumping sobre funder + ward (primer cuartil)",
  "Num + Cat (> 1 & < 2100) fe anteriores + lumping (mediana) + hashed sobre funder + ward",
  "Num + Cat (> 1 & < 2100) fe anteriores + lumping (mediana) + freq. abs. sobre funder + ward",
  "Num + Cat (> 1 & < 2100) fe anteriores + lumping (mediana) + freq. abs. y target enc. (media) sobre funder + ward",
  "Num + Cat (> 1 & < 2100) fe anteriores + lumping (mediana) + freq. abs. y target enc. (suma) sobre funder + ward",
  "Num + Cat (> 1 & < 2100) fe anteriores + lumping (mediana) + word embed sobre funder y ward (dim = 2)",
  "Num + Cat (> 1 & < 2100) fe anteriores + lumping (mediana) sobre funder y ward + freq. abs. cat.",
  "Num + Cat (> 1 & < 10000) fe anteriores + lumping (mediana) + freq. abs. cat. (sin installer)",
  "Num + Cat (> 1 & < 10000) fe anteriores + lumping (mediana) + freq. abs. cat. (sin installer) + variables logicas"))


color_me <- which(puntuaciones$Driven.Data == 0.8251)
kable(puntuaciones) %>%
  kable_styling() %>%
  row_spec(color_me, bold = TRUE, background = "#4D934D")
```

### 3.1.13 Extracción de variables sobre date_recorded

#### Nuevas variables extraidas de date_recorded

Tras añadir las variables lógicas _public_meeting_ y _permit_, hemos utilizado todas las variables disponibles hasta el momento. No obstante, una de los campos que, en mi opinión, no llegó a aprovecharse lo suficiente, fue _date_recorded_, del cual solo mantuvimos los campos:

1. _fe_month_: mes de _date_recorded_
2. _fe_dr_year_cyear_diff_: diferencia entre el año de _date_recorded_ y el año de construcción.

No obstante, de dicho campo podemos aprovechar mucho más, extrayendo diferentes variables como las que se proponen a continuación:

1. _year_: durante el concurso en clase, acabamos descartando dicha variable al no aportar mejoría alguna al modelo. Sin embargo, con el _dataset_ depurado hasta ahora ¿Mejorará si lo incluimos?

2. Podemos incluso añadir nuevas variables a partir de _date_recorded_ como el __día__, el __día de la semana__, el __día del cuatrimestre__, la __semana__, el __cuatrimestre__ o incluso __si el día en el que se registró la bomba de agua era fin de semana o no__.

Comenzando añadiendo nuevamente el campo _year_ al _dataset_ depurado hasta el momento, haciendo uso de la librería _lubridate_:

```{r, eval=FALSE}
#-- Icluimos year
year <- c(year(datcat_mas_1000_mas_logicas_dr[, date_recorded]))
datcompleto_imp$fe_dr_year <- year
```


```{r, echo=FALSE}
kable(data.frame("Train accuracy" = c(0.8168855, 0.8173064), 
                        "Driven Data" = c(0.8251, 0.8236),
  row.names = c("Mejor modelo hasta el momento",
  "Añadiendo year (date_recorded)")),
      align = 'c') %>%
  kable_styling() %>%
  row_spec(1:2, background = "white")
```

Nuevamente, nos encontramos con que la variable _year_ no aporta mejoría alguna al modelo.

En contraposición, tal y como se mencionó anteriormente, podemos añadir nuevas variables extraídas a partir de _date_recorded_, gracias al paquete _lubridate_ de _tidyverse_:

```{r, eval=FALSE}
#-- Incluimos otros campos, mediante el paquete lubridate
datcompleto_imp[, fe_dr_day :=  day(date_recorded)]        # Dia
datcompleto_imp[, fe_dr_wday := wday(date_recorded)]       # Dia de la semana
datcompleto_imp[, fe_dr_qday := qday(date_recorded)]       # Dia del cuatrimestre
datcompleto_imp[, fe_dr_week := week(date_recorded)]       # Semana
datcompleto_imp[, fe_dr_quarter := quarter(date_recorded)] # Cuatrimestre

#-- Incluimos si es o no fin de semana
datcompleto_imp[, fe_is_weekend := ifelse(fe_dr_wday %in% c(1,7), 1, 0)]
```

```{r, echo=FALSE}
kable(data.frame("Train accuracy" = c(0.8168855, 0.8173064, 0.8165657), 
                        "Driven Data" = c(0.8251, 0.8236, 0.8224),
  row.names = c("Mejor modelo hasta el momento",
  "Añadiendo year (date_recorded)",
  "Incluyendo otros campos")),
      align = 'c') %>%
  kable_styling() %>%
  row_spec(1:3, background = "white")
```

Pese a incluir tantos campos, el modelo empeora. Analicemos el gráfico de importancia:

```{r, echo=FALSE, fig.width=6, fig.height=6}
include_graphics("./charts/20_lumping_fe_freq_abs_sobre_funder_ward_scheme_name_resto_categoricas_mas_logicas_fe_otros_campos.png")
```

__Nota: para una mejor visualización, se han eliminado el resto de variables__.

Podemos observar que variables como el cuatrimestre o si es o no fin de semana (fe_is_weekend y fe_dr_quarter)  __no presentan una importancia demasiado relevante__, lo que podría estar provocando una disminución del _score_ final ¿Mejoraría el modelo si descartamos ambas columnas?

```{r, echo=FALSE}
kable(data.frame("Train accuracy" = c(0.8168855, 0.8173064, 0.8165657, 0.8171212), 
                        "Driven Data" = c(0.8251, 0.8236, 0.8224, 0.8226),
  row.names = c("Mejor modelo hasta el momento",
  "Añadiendo year (date_recorded)",
  "Incluyendo otros campos",
  "Solo dr_qday, dr_day, dr_week y dr_wday")),
      align = 'c') %>%
  kable_styling() %>%
  row_spec(1:4, background = "white")
```

Descartando ambas variables, el modelo mejora ligeramente, aunque continua lejos del mejor _score_ (0.8251). No obstante, podemos dar un salto más y eliminar las siguientes variables con menor importancia como el día de la semana ( _fe_dr_wday_ ) y la semana ( _fe_dr_week_ ):

```{r, echo=FALSE}
kable(data.frame("Train accuracy" = c(0.8168855, 0.8173064, 0.8165657, 0.8171212, 0.8175084), 
                        "Driven Data" = c(0.8251, 0.8236, 0.8224, 0.8226, 0.8242),
  row.names = c("Mejor modelo hasta el momento",
  "Añadiendo year (date_recorded)",
  "Incluyendo otros campos",
  "Solo dr_qday, dr_day, dr_week y dr_wday",
  "Solo dr_qday y dr_day")),
      align = 'c') %>%
  kable_styling() %>%
  row_spec(1:5, background = "white")
```

Aunque bien es cierto que el modelo mejora de 0.8226 a 0.8242, continua estando lejos de 0.8251, incluso si solo mantuviéramos _fe_dr_qday_:

```{r, echo=FALSE}
kable(data.frame("Train accuracy" = c(0.8168855, 0.8173064, 0.8165657, 0.8171212, 0.8175084, 0.8170202), 
                        "Driven Data" = c(0.8251, 0.8236, 0.8224, 0.8226, 0.8242, 0.8243),
  row.names = c("Mejor modelo hasta el momento",
  "Añadiendo year (date_recorded)",
  "Incluyendo otros campos",
  "Solo dr_qday, dr_day, dr_week y dr_wday",
  "Solo dr_qday y dr_day",
  "Solo dr_qday")),
      align = 'c') %>%
  kable_styling() %>%
  row_spec(1:6, background = "white")
```

El _score_ apenas mejora. Por tanto, __descartamos añadir cualquiera de las variables anteriores al modelo actual__.

#### Day_count

Hemos comprobado como, de todas las posibles variables que podían extraerse de _date_recorded_, las únicas que hemos mantenido han sido _fe_dr_year_cyear_diff_ y _fe_dr_month_. No obstante, y en relación a _date_recorded_, podríamos crear una nueva variable, __similar a lo que realizamos en clase con construction_year__ (restando a la variable un año de referencia: 2014). 

Para empezar, __obtenemos una fecha de referencia, la fecha más reciente de la columna _date_recorded___:

```{r}
#-- Obtenemos una fecha de referencia (mas reciente)
date_recorded    <- c(datcat_mas_1000_mas_logicas_dr$date_recorded)
fecha_referencia <- max(ymd(date_recorded)) # Tenemos 2013-12-03
fecha_referencia
```

Sabemos que la máxima fecha registrada corresponde con el 3 de diciembre del año 2013 __¿Y si creamos una nueva variable que almacenen la diferencia (en días) entre dicha fecha y los valores de _date_recorded_?__

```{r, eval=FALSE}
datcompleto_imp[fe_dr_day_count := as.numeric(fecha_referencia - ymd(date_recorded))]
```

```{r, echo=FALSE}
puntuaciones <- data.frame("Train accuracy" = c(0.8168855, 0.8168855), 
                        "Driven Data" = c(0.8251, 0.8247),
  row.names = c("Mejor modelo hasta el momento",
  "Añadiendo day_count"))

color_me <- which(puntuaciones$Driven.Data == 0.8251)
kable(puntuaciones) %>%
  kable_styling() %>%
  row_spec(color_me, bold = TRUE, background = "#4D934D")
```

Pese al añadir dicha variable, el modelo continua sin mejorar.

### Conclusión Feature Engineering

Tras realizar todos los pasos anteriores, dado el mejor _score_ obtenido, nos decantamos por:

1. Selección de variables numéricas

2. Selección de variables categóricas (con menos de 10000 categorías), __a excepción de _installer___.

3. Aplicar la función _clean_text_ sobre las variables categoricas (conversión a minúsculas, eliminación de espacios en blanco y signos de puntuación).

4. Aplicar _lumping_ sobre las variables categóricas, __concretamente sobre la mediana en proporción de aparición__.

5. Añadir las variables lógicas _permit_ y _public_meeting_, imputando NAs por _missRanger_.

__Mejor _score_ obtenido en DrivenData__: __0.8251__.

## 3.2 Tuneo de modelos

A continuación, y en base al _dataset_ modelado por los pasos anteriores, se propone el tuneo de tres modelos _Machine Learning_:

1. _Random Forest_

2. _XGboost_

3. _autoML_ (h2o)

### 3.2.1 Random Forest

Durante la etapa de _Feature Engineering_, empleamos la siguiente configuración:

1. Número de árboles: 500

2. _mtry_: por defecto (raíz cuadrada del número de variables, alrededor de 6)

El tuneo mediante _Random Forest_ se divide en dos etapas:

#### 3.2.1.1. Selección del número de árboles, __empleando el valor _mtry_ por defecto__:

```{r, eval=FALSE}
#-- Tuneo numero de arboles
n_trees <- c(400, 500, 600, 700, 800, 900)

prediction_errors <- c()
for (n_tree in n_trees) {
  rf  <- fit_random_forest(formula, train, num_trees = n_tree)
  prediction_error <- rf$prediction.error
  prediction_errors <- c(prediction_errors, 1 - prediction_error)
  
  submission <- make_predictions(rf, test)
  fwrite(submission, paste0("./submissions/tunning_models/random_forest/ntrees/random_forest_with_",n_tree,"_ntrees.csv"))
}
rm(n_tree); rm(prediction_error); rm(rf); rm(submission)
```


```{r, echo=FALSE}
include_graphics("./charts/random_forest_seleccion_ntree.png")
```

Aparentemente, podemos comprobar que 400-500 arboles es una buena opcion. Sin embargo, por cuestiones asociadas a la reproducibilidad de la semilla, el _score_ del modelo aumenta de 0.8251 a 0.8253.

#### 3.2.1.2. Selección del mejor valor _mtry_:

Dado que con 400-500 árboles se obtiene un _score_ superior al obtenido en la fase de _Feature Engineering_, realizamos el tuneo del valor _mtry_ sobre ambos:

__Empleando 400 árboles__:

```{r, echo=FALSE}
include_graphics("./charts/random_forest_seleccion_ntree400_mtry_tunning.png")
```

__Empleando 500 árboles__:

```{r, echo=FALSE}
include_graphics("./charts/random_forest_seleccion_ntree500_mtry_tunning.png")
```

En ambos casos, __pese a aumentar o disminuir el valor _mtry_, el modelo no mejora en cuanto a _score_ se refiere__.

#### 3.2.1.3. Tuneado de la profundidad del arbol (max_depth)

Por último, procedemos a tunear __la profundidad máxima del árbol__, cuyo valor por defecto es NULL, es decir, __profundidad ilimitada__:

```{r, eval=FALSE}
#-- Veamos si tuneando la profundidad maxima del arbol mejora
max_depth_vector           <- c(200, 150, 80, 50)
prediction_errors <- c()
for (max_depth in max_depth_vector) {
  # Empleamos 500 arboles y mtry = 6
  rf  <- fit_random_forest(formula, train, num_trees = 500, mtry = 6, max_depth = max_depth)
  prediction_error <- rf$prediction.error
  prediction_errors <- c(prediction_errors, 1 - prediction_error)
  submission <- make_predictions(rf, test)
  fwrite(submission, paste0("./submissions/tunning_models/random_forest/max_depth/random_forest_with_",max_depth,"_max_depth.csv"))
}
rm(n_tree); rm(prediction_error); rm(rf); rm(submission)
```

```{r, echo=FALSE}
include_graphics("./charts/random_forest_seleccion_max_depth.png")
```

Curiosamente, __empleando una profundidad máxima de 50-80__, el _score_ del modelo se estabiliza en torno a 0.8253, mismo _score_ que el obtenido con profudidad ilimitada. Por tanto, con _Random Forest_ mantendremos la configuración original:

```{r, echo=FALSE}
puntuaciones <- data.frame("Driven Data" = c(0.8251, 0.8253),
  row.names = c("Mejor score Feature Egineering",
  "Mejor score Random Forest"))

color_me <- which(puntuaciones$Driven.Data == 0.8253)
kable(puntuaciones) %>%
  kable_styling() %>%
  row_spec(color_me, bold = TRUE, background = "#4D934D")
```

### 3.2.2 XGboost

Continuando con XGboost, nos encontramos con los siguientes parámetros a tunear:

1. _nrounds_ o número de iteraciones.
2. _max_depth_ o profundidad máxima del árbol.
3. _eta_ o parámetro de regularización.
4. _colsample_bytree_ o número de variables a sortear por cada iteración.
5. _colsample_bylevel_ o número de variables a sortear por cada nuevo nivel.
6. _colsample_bynode_ o número de variables a sortear cada vez que se tenga que realizar una división en el árbol.
7. _subsample_ o porcentaje de "submuestras" a escoger durante el entrenamiento.

Nuevamente, para el tuneo del XGboost __comenzamos tuneando el número de iteraciones o _nrounds___. No obstante, y dado que con un _mtry_ bajo obtuvimos buenos resultados en _Random Forest_, de cara a XGboost __realizamos un sorteo de columnas a través de la variable _colsample_bytree___, concretamente un porcentaje bajo (0.3), __de forma que se sortee un pequeño subconjunto de variables (30 %) en lugar de todas las columnas (por defecto)__. Además, de cara a obtener un _accuracy_ "aproximado" del modelo, __empleamos _mlogloss_ como métrica de evaluación (por defecto)__:


```{r}
params = list(
  objective = "multi:softmax",
  num_class = 3,
  colsample_bytree = 0.3,  # Sorteamos el 30 % de las variables
  eval_metric = "mlogloss" # multiclass logloss
)
```

#### 3.2.2.1. Tuneo del número de iteraciones

Comenzamos tuneando el número de iteraciones o _nrounds_:

```{r, eval=FALSE}
#-- Modelo 1: parametrizando el numero de iteraciones
nrounds     <- c(400, 500, 600, 700)
lista_accuracy <- c()
for(nround in nrounds) {
  
  my_model <- fit_xgboost_model(params, xgb.train, xgb.train, nrounds = nround)
  xgb_pred <- make_predictions_xgboost(my_model, test)
  fwrite(xgb_pred, 
      file = paste0("./submissions/tunning_models/xgboost/rounds/xgboost_with_",nround,"_iters.csv")
  )

  # "accuracy" = 1 - mlogloss
  lista_accuracy <- c(lista_accuracy, 1 - tail(my_model$evaluation_log$val1_mlogloss, 1))
}
```

```{r, echo=FALSE}
include_graphics("./charts/xgboost_nrounds.png")
```

Como podemos comprobar en la salida anterior, tuneando el número de iteraciones obtenemos una puntuación máxima de 0.816 en el conjunto _test_, un _score_ bajo en comparación con los obtenidos tanto en _Feature Engineering_ como en _Random Forest_. 

#### 3.2.2.2. Tuneo de _max_depth_ y _eta_:

A continuación, y utilizando el mejor número de iteraciones (500), __procedemos a tunear otros parámetros fundamentales en el modelo__, concretamente:

1. __Profundidad máxima del árbol__ o _max_depth_.

2. __Parámetro de regularización__ o  _eta_.

```{r, eval=FALSE}

search_grid <- expand.grid(colsample_bytree = c(0.3),
                           max_depth = c(20, 15, 10, 8, 6, 3),
                           eta = c(0.3, 0.4, 0.5)
)
```

```{r, echo=FALSE}
include_graphics("./charts/xgboost_max_depth_ntrees.png")
```

En primera instancia, empleando una profundidad máxima de 15 en el árbol, así como un parametro de regularización en torno a 0.3, conseguimos mejorar el resultado, __aunque no lo suficiente en comparación con el mejor _score_ obtenido hasta el momento__: 0.8253. 

Sin embargo, de la gráfica anterior debemos detallar un aspecto fundamental: __a medida que se reduce el parametro de regularización _eta_, el _score_ del _test_ aumenta__. Por tanto, ¿Y si reducimos aún más dicho parámetro? Veamos los resultados, __empleando tanto 500 iteraciones como una profundidad máxima de 15__ (mejores valores hasta el momento):

```{r, echo=FALSE}
include_graphics("./charts/xgboost_eta_parameter.png")
```

Como podemos observar, __reduciendo parámetro _eta_ hasta 0.02 ha permitido mejorar el _score_ del conjunto _test_ a 0.8257__, superior al obtenido por _Random Forest_. Sin embargo, debemos recordar un detalle relevante: __a medida que disminuye el parámetro de regularización, el tiempo de cómputo aumenta, por lo que puede ser necesario añadir más iteraciones__. Por tanto, probamos a continuación modelos XGboost variando no solo el parámetro _eta_ (entre 0.01 y 0.04, donde se han obtenido los resultados más altos), sino además variando el número de iteraciones (entre 500 y 700):

```{r, echo=FALSE}
include_graphics("./charts/xgboost_nrounds_eta_parameter_tunned.png")
```

Comprobamos como con 600 iteraciones y un parametro de regularización de 0.02 __conseguimos mejorar aun más el modelo, de 0.8257 a 0.8260__.

#### 3.2.2.3 Tuneo de colsample_bytree

A continuación, y empleando 600 iteraciones, _max_depth_ = 15 y _eta_ = 0.02, procedemos a tunear el parámetro _colsample_bytree_, el cual hemos mantenido por defecto a 0.3. De este modo, __comprobamos si sorteando un mayor o menor número de variables mejora el _score_ del modelo__:

```{r, echo=FALSE}
include_graphics("./charts/xgboost_colsample_by_tree.png")
```

Pese a aumentar o disminuir el sorteo de variables, el _score_ del modelo no mejora, __por lo que mantenemos la variable _colsample_bytree_ a 0.3__.

#### 3.2.2.4 Tuneo de subsample

Por último, tunearemos un últimos parámetro: _subsample_, cuyo valor por defecto es 1, es decir, __se emplean todas las observaciones durante en el entrenamiento__. Veamos si mejora el modelo al variar el tamaño de la muestra de entrenamiento:

```{r, echo=FALSE}
include_graphics("./charts/xgboost_subsample.png")
```

Nuevamente, ajustando el tamaño de la muestra de entrenamiento, __el modelo continua sin mejorar__.

#### 3.2.2.5 Tuneo de colsample_bylevel y colsample_bynode

En último lugar, nos queda por tunear los parámetros _colsample_bylevel_ y _colsamle_bynode_, los cuales permiten sortear las variables a utilizar por cada nivel y nodo del árbol, respectivamente. Dichos parámetros son __acumulativos__. Pongamos un ejemplo:

1. _colsample_bytree_: 0.3. __Sorteo un 30 % de las variables en la construcción de cada árbol__.

2. _colsample_bylevel_: 0.7. Del 30 % anterior, se sortean un 70 % de las variables por cada nuevo nivel de profundidad, es decir, _total_variables_ * 0.3 * 0.7.

3. _colsample_bynode_: 0.6. Del _total_variables_ * 0.3 * 0.7 anterior, se sortea un 60 % de las variables cada vez que se tenga que realizar una división en el árbol.

Por ello, del 30 % de variables sorteadas en cada arbol, __probamos a sortear un porcentaje de variables en cada nivel__, tuneando _colsample_bylevel_ a 0.9, 0.8 o 0.7:

```{r, echo=FALSE}
puntuaciones <- data.frame("Driven Data" = c(0.8260, 0.8265, 0.8257, 0.8260),
  row.names = c("colsample_bytree = 0.3",
  "colsample_bytree = 0.3 & colsample_bylevel = 0.9",
  "colsample_bytree = 0.3 & colsample_bylevel = 0.8",
  "colsample_bytree = 0.3 & colsample_bylevel = 0.7"))

color_me <- which(puntuaciones$Driven.Data == 0.8265)
kable(puntuaciones) %>%
  kable_styling() %>%
  row_spec(color_me, bold = TRUE, background = "#4D934D")
```

Curiosamente, __sorteando además el número de observaciones en cada nivel, el modelo mejora a 0.8265__ ¿Y si damos un más y sorteamos también el número de observaciones en cada división del árbol?

```{r, echo=FALSE}
puntuaciones <- data.frame("Driven Data" = c(0.8265, 0.8256, 0.8244, 0.8272),
  row.names = c("colsample_bytree = 0.3 & colsample_bylevel = 0.9",
  "colsample_bytree = 0.3 & colsample_bylevel = 0.9 & colsample_bynode = 0.9",
  "colsample_bytree = 0.3 & colsample_bylevel = 0.9 & colsample_bynode = 0.8",
  "colsample_bytree = 0.3 & colsample_bylevel = 0.9 & colsample_bynode = 0.7"))

color_me <- which(puntuaciones$Driven.Data == 0.8272)
kable(puntuaciones) %>%
  kable_styling() %>%
  row_spec(color_me, bold = TRUE, background = "#4D934D")
```

__Incluso sorteando las variables a tan bajo nivel, ¡Sigue mejorando el modelo!__ ¿Y si reducimos aún más _colsample_bynode_?

```{r, echo=FALSE}
puntuaciones <- data.frame("Driven Data" = c(0.8265, 0.8256, 0.8244, 0.8272, 0.8261, 0.8266),
  row.names = c("colsample_bytree = 0.3 & colsample_bylevel = 0.9",
  "colsample_bytree = 0.3 & colsample_bylevel = 0.9 & colsample_bynode = 0.9",
  "colsample_bytree = 0.3 & colsample_bylevel = 0.9 & colsample_bynode = 0.8",
  "colsample_bytree = 0.3 & colsample_bylevel = 0.9 & colsample_bynode = 0.7",
  "colsample_bytree = 0.3 & colsample_bylevel = 0.9 & colsample_bynode = 0.6",
  "colsample_bytree = 0.3 & colsample_bylevel = 0.9 & colsample_bynode = 0.5"))

color_me <- which(puntuaciones$Driven.Data == 0.8272)
kable(puntuaciones) %>%
  kable_styling() %>%
  row_spec(color_me, bold = TRUE, background = "#4D934D")
```

Aparentemente, 0.7 es una buena opción. Como conclusión, con _XGboost_ empleamos la siguiente configuración:

1. _colsample_bytree_: 0.3
2. _max_depth_: 15
3. _eta_: 0.02
4. _nrounds_: 600
5. _subsample_: 1
6. _colsample_bylevel_: 0.9
7. _colsample_bynode_: 0.7

```{r, echo=FALSE}
puntuaciones <- data.frame("Driven Data" = c(0.8251, 0.8253, 0.8272),
  row.names = c("Mejor score Feature Egineering",
  "Mejor score Random Forest",
  "Mejor score XGboost"))

color_me <- which(puntuaciones$Driven.Data == 0.8272)
kable(puntuaciones) %>%
  kable_styling() %>%
  row_spec(color_me, bold = TRUE, background = "#4D934D")
```

#### 3.2.1 XGboost con H2o

```{r, echo=FALSE, out.width="15%", out.height="15%"}
include_graphics("./h2o_ai.png")
```

Tras tunear el modelo con el paquete XGboost original, __¿Que ocurriría si trasladamos los parámetros del mejor modelo a otra librería?__ Concretamente, H2o dispone de una función denominada _h2o.xgboost_, el cual permite tunear prácticamente los mismos parámetros que en el paquete anterior:

```{r, eval=FALSE}
#-- Conversion train y test a objeto h2o 
train_h <- as.h2o(train)
test_h  <- as.h2o(test)

xgboost_model <- h2o.xgboost(x = pred, y = y,
                             training_frame = train_h, ntrees = 600, 
                             seed = 1234, distribution = "multinomial",
                             eta = 0.02, max_depth = 15, colsample_bytree = 0.3, 
                             colsample_bylevel = 0.9, colsample_bynode = 0.7)
```

```{r, echo=FALSE}
puntuaciones <- data.frame("Driven Data" = c(0.8272, 0.8255),
  row.names = c("Mejor score XGboost (paquete xgboost)",
  "Mejor score XGboost (paquete H2o)"))

color_me <- which(puntuaciones$Driven.Data == 0.8272)
kable(puntuaciones) %>%
  kable_styling() %>%
  row_spec(color_me, bold = TRUE, background = "#4D934D")
```

Dado que se tratan se diferentes paquetes, los resultados del modelo son ligeramente diferentes. En este caso, el resultado con h2o es menor, por lo que nos decantamos por el paquete XGboost anterior.

#### 3.2.2 Autoxgboost

```{r, echo=FALSE, out.width="15%", out.height="15%"}
include_graphics("./hexagon_logo.svg")
```

Antes de finalizar con _XGboost_ realizamos una última prueba, concretamente con un paquete denominado _autoxgboost_, desarrollado por _Janek Thomas_. Dicho paquete permite realizar, de un modo similar a _h2o_, el tuneo automático del mejor modelo a través de una optimización bayesiana:

```{r, eval=FALSE}
#-- autoxgboost
bomb_task <- makeClassifTask(data = as.data.frame(train), target = "status_group")

#  max.rounds:  numero maximo de iteraciones por modelo (indicamos un maximo de 600, el mejor
#               parametro obtenido hasta el momento)
#  time.budget: tiempo maximo de tuneo (lo establecemos a media hora: 1800 segundos)
my_model  <- autoxgboost(bomb_task, max.nrounds = 600, time.budget = 1800)

submission <- make_predictions(model = my_model, test_data = as.data.frame(test))
```

```{r, echo=FALSE}
puntuaciones <- data.frame("Driven Data" = c(0.8251, 0.8253, 0.8272, 0.8206),
  row.names = c("Mejor score Feature Egineering",
  "Mejor score Random Forest",
  "Mejor score XGboost",
  "autoxgboost"))

color_me <- which(puntuaciones$Driven.Data == 0.8272)
kable(puntuaciones) %>%
  kable_styling() %>%
  row_spec(color_me, bold = TRUE, background = "#4D934D")
```

Pese al tuneo realizado, continua siendo mejor el modelo obtenido en el apartado anterior.

### 3.3 AutoML con H2o

Finalmente, con los datos depurados en el apartado _Feature Engineering_ elaboramos modelos _automl_ empleando el paquete _h2o_:

```{r, eval=FALSE}
library(h2o)  # AutoML
#-- Iniciamos el cluster h2o
h2o.init()

#-- Conversion train y test a objeto h2o 
train_h <- as.h2o(train)
test_h  <- as.h2o(test)

#-- Lanzamos el modelo
#   1. Probamos con 10 modelos
#   2. Excluiremos los algoritmos mas costosos
#      2.1 Deep Learning
#      2.2 Ensamblado
#   3. Extraemos los 10 mejores modelos
#   4. Indicamos un balanceo de clases en la validacion cruzada
aml <- h2o.automl(x = pred, y = y,
                 training_frame = train_h,
                 max_models = 5,
                 seed = 1234,
                 exclude_algos = c("DeepLearning", "StackedEnsemble"),
                 balance_classes = TRUE
)

#-- Realizamos las predicciones y guardamos submissions
model_id <- as.vector(aml@leaderboard$model_id)
for(i in 1:length(model_id)) {
  aml_aux <- h2o.getModel(aml@leaderboard[i, 1])
  prediccion    <- h2o.predict(aml_aux, newdata = test_h)
  prediccion_df <- as.data.frame(prediccion)
  
  prediccion_df    <- prediccion_df$predict
  submission       <- data.table(id = test$id, status_group = prediccion_df)
  path             <- paste0("./submissions/tunning_models/h2o/",model_id[i],".csv")
  fwrite(submission, path)
}
```

Tras elaborar los modelos _automl_, analicemos las _submissions_:

```{r, echo=FALSE}
puntuaciones <- data.frame("Driven Data" = c(0.8251, 0.8253, 0.8272, 0.8206, 0.8101, 
                                                 0.8118, 0.8020, 0.8079, 0.8137),
                row.names = c("Mejor score Feature Egineering",
                "Mejor score Random Forest",
                "Mejor score XGboost",
                "autoxgboost",
                "XGBoost_2_AutoML",
                "XGBoost_1_AutoML",
                "XGBoost_3_AutoML",
                "GBM_5_AutoML",
                "GBM_4_AutoML"))

color_me <- which(puntuaciones$Driven.Data == 0.8272)
kable(puntuaciones) %>%
  kable_styling() %>%
  row_spec(color_me, bold = TRUE, background = "#4D934D")
```

Analizando las salidas anterioes, __ninguno de los modelos _automl_ obtenidos mejora el _score_ del apartado anterior__.

### Conclusiones

Finalmente, y en base a los resultados obtenidos a lo largo de la práctica, __el mejor _score_ obtenido ha sido de 0.8272__:

```{r, echo=FALSE}
include_graphics("./submission.png")
```

# 4. Anexo: código final

A continuación, se incluye el código final con el mejor _score_:

```{r, eval=FALSE}
suppressPackageStartupMessages({
  library(dplyr)                # Manipulacion de datos 
  library(data.table)           # Lectura y escritura de ficheros
  library(ranger)               # randomForest (+ rapido que caret)
  library(forcats)              # Tratamiento de variables categoricas
  library(tictoc)               # Calculo de tiempo de ejecucion
  library(missRanger)           # Imputacion de valores NA
  library(knitr)                # Generacion de informes (formateo de tablas)
  library(gmt)                  # Calculo de la distancia geografica
  library(stringi)              # Tratamiento de strings
  library(missRanger)           # Tratamiento de valores missing (mediante random forest)
  library(xgboost)              # XGboost
})

# -----------------------------
# ----- funciones propias -----
# -----------------------------

#-- Funcion para limpieza de textos
#   1. Conversion a minusculas
#   2. Eliminacion de espacios en blanco
#   3. Eliminacion de signos de puntuacion
clean_text <- function(text) {
  stri_trans_tolower(
    stri_replace_all_regex(
      text, 
      pattern = "[ +\\p{Punct}]", 
      replacement = ""
    )
  )
}

#-- Funcion para entrenar un modelo XGboost, en base a los parametros proporcionados
fit_xgboost_model <- function(params, train, val, nrounds, early_stopping_rounds = 20, show_log_error = TRUE, seed = 1234) {
  set.seed(seed)
  my_model <- xgb.train(
    data   = train,
    params = params,
    watchlist=list(val1=val),
    verbose = 1,
    nrounds= nrounds,
    early_stopping_rounds = early_stopping_rounds,
    nthread=4
  )
  return(my_model)
}

#-- Funcion para realizar la prediccion de un modelo XGboost
make_predictions_xgboost <- function(my_model, test) {
  xgb_pred <- predict(my_model,as.matrix(test),reshape=T)
  xgb_pred <- ifelse(xgb_pred == 0, "functional", 
                     ifelse(xgb_pred == 1, 
                            "functional needs repair", 
                            "non functional"
                            )
                     )
  
  xgb_pred <- data.table(id = test$id, status_group = xgb_pred)
  return(xgb_pred)
}

# -----------------------------
# -------- script final -------
# -----------------------------

#---------------------- Carga de ficheros train y test ---------------------
dattrainOr    <- fread(file = "./data/train_values.csv", data.table = FALSE )
dattrainLabOr <- fread(file = "./data/train_labels.csv", data.table = FALSE )
dattestOr     <- fread(file = "./data/test_values.csv", data.table  = FALSE )

#-------------------------- Variables categoricas ---------------------------
datcat_df <- dattrainOr %>% select(where(is.character))

# Mediante un bucle for... 
numlev_df <- data.frame(
  "vars" = names(datcat_df),
  "levels" = apply(datcat_df, 2, 
                   function(x) length(unique(x)))
  
)

# Eliminamos los nombres de fila
rownames(numlev_df) <- NULL

numlev_df %>% arrange(levels)

# Unimos dattrainOr con la variable objetivo
dattrainOrlab <- merge(
  dattrainOr, dattrainLabOr,
  by.x = c('id'), by.y = c('id'),
  sort = FALSE
)

#-- Eliminamos las variables no empleadas
dattrainOrlab$recorded_by    <- NULL; dattestOr$recorded_by    <- NULL
dattrainOrlab$payment_type   <- NULL; dattestOr$payment_type   <- NULL
dattrainOrlab$quantity_group <- NULL; dattestOr$quantity_group <- NULL
dattrainOrlab$installer      <- NULL; dattestOr$installer      <- NULL
dattrainOrlab$wpt_name       <- NULL; dattestOr$wpt_name       <- NULL
dattrainOrlab$subvillage     <- NULL; dattestOr$subvillage     <- NULL

vector_status_group <- dattrainOrlab$status_group

dattrainOrlab$status_group <- NULL
datcompleto <- as.data.table(rbind(dattrainOrlab, dattestOr))
#-- Guardamos el indice de fila donde comienza el conjunto "test",
#   concretamente la posicion 59401
fila_test <- which(datcompleto$id == 50785)

#----------------------------- Feature Engineering ------------------------------
#-- fe_cyear: 2014 - construction_year
datcompleto$fe_cyear     <- 2014 - datcompleto$construction_year

#-- fe_dist: geodist(latitude, longitude) al (0,0)
datcompleto$fe_dist <- geodist(datcompleto$latitude, datcompleto$longitude, 0, 0)

#-- fe_cant_agua: cantidad de agua / hab.
datcompleto$fe_cant_agua <- ifelse(datcompleto$population == 0,
                                         0,
                                         round(datcompleto$amount_tsh /
                                                 datcompleto$population, 3)
)

#-- month: mes date_recorded
datcompleto$fe_dr_month           <- month(datcompleto$date_recorded)

#-- fe_dr_year_cyear_diff: año date_recorded - construction_year
datcompleto$fe_dr_year_cyear_diff <- year(datcompleto$date_recorded) - datcompleto$construction_year

#-- Eliminamos date_recorded
datcompleto$date_recorded <- NULL

#-- Limpieza de variables categoricas mediante clean_text
cols <- c('funder', 'ward', 'scheme_name')
datcompleto[ , paste0('fe_',cols) := lapply(.SD, clean_text), .SDcols = cols]
rm(cols)

#-- lumping sobre la mediana de la proporcion de aparicion
#-  fe_funder
summary(c(prop.table(table(datcompleto[, fe_funder]))))
datcompleto[, fe_funder := fct_lump_prop(datcompleto[,fe_funder], 2e-05, other_level = "other")]
datcompleto$fe_funder <- as.character(datcompleto$fe_funder)

datcompleto[, funder := NULL]

#- fe_ward
summary(c(prop.table(table(datcompleto[, fe_ward]))))
datcompleto[, fe_ward := fct_lump_prop(datcompleto[,fe_ward], 4e-04, other_level = "other")]
datcompleto$fe_ward <- as.character(datcompleto$fe_ward)

datcompleto[, ward := NULL]

#- fe_scheme_name
summary(c(prop.table(table(datcompleto[, fe_scheme_name]))))
datcompleto[, fe_scheme_name := fct_lump_prop(datcompleto[,fe_scheme_name], 8.1e-05, other_level = "other")]
datcompleto$fe_scheme_name <- as.character(datcompleto$fe_scheme_name)

datcompleto[, scheme_name := NULL]

#-- Imputacion de las variables categoricas por sus frecuencias absolutas
cat_cols <- names(datcompleto[, which(sapply(datcompleto, is.character)), with = FALSE])

#-  Antes de imputar
freq_antes_fe <- apply(datcompleto[, ..cat_cols], 2, function(x) length(unique(x)))

for (cat_col in cat_cols) {
  datcompleto[, paste0("fe_", cat_col) := as.numeric(.N), by = cat_col]
}
names(datcompleto) <- stri_replace_all_fixed(names(datcompleto),
                                             "fe_fe_", "fe_")
#-- Eliminamos las variables originales
for (cat_col in cat_cols) {
  datcompleto[, paste(cat_col) := NULL]
}

new_cat_cols <- paste0("fe_", stri_replace_all_fixed(cat_cols, "fe_", ""))

#- Despues de imputar
freq_despues_fe <- apply(datcompleto[, ..new_cat_cols], 2, function(x) length(unique(x)))

cat("ANTES DE IMPUTAR")
freq_antes_fe
cat("------------------------------")
cat("DESPUES DE IMPUTAR")
freq_despues_fe

#-- Variables logicas (debemos imputarlas, dado que en el conjunto test existen missings)
sum(is.na(dattestOr$permit))
sum(is.na(dattestOr$public_meeting))

# Ordenamos las columnas para obtener la misma imputacion de las variables logicas con missRanger
# dado que al variar el orden de las columnas, la imputacion tambien varia
cols_orden <- c(names(datcompleto)[c(1:9)],  "construction_year", "fe_cyear", 
                "fe_dist", "fe_cant_agua", "fe_dr_year_cyear_diff", "fe_dr_month", 
                "public_meeting", "permit", names(datcompleto)[c(18:38)])

setcolorder(datcompleto, cols_orden)

#-- Imputamos por missRanger
datcompleto_imp <- missRanger(datcompleto,
                              pmm.k = 5,
                              seed = 1234,
                              maxiter = 100)

# Comprobamos que las variables logicas estan imputadas
sum(is.na(datcompleto_imp))

# Las convertimos a variables numericas
datcompleto_imp[, public_meeting := as.numeric(public_meeting)]
datcompleto_imp[, permit := as.numeric(permit)]

#-- Creamos dos columnas adicionales que indiquen si la variable logica era o no NA:
#   is_na_public_meeting
#   is_na_permit
datcompleto_imp[, is_na_public_meeting := ifelse(is.na(datcompleto[, public_meeting]), 1, 0)]
datcompleto_imp[, is_na_permit := ifelse(is.na(datcompleto[, permit]), 1, 0)]
#---------------------------- Fin Feature Engineering -----------------------------

#------------------------------------ Modelo --------------------------------------
formula <- as.formula("status_group~.")

train <- datcompleto_imp[c(1:fila_test-1),]

test  <- datcompleto_imp[c(fila_test:nrow(datcompleto_imp)),]

# Transformamos la variable objetivo en numerica
vector_status_group <- ifelse(vector_status_group == "functional", 0
                              , ifelse(vector_status_group == "functional needs repair", 1
                              , 2))
xgb.train <- xgb.DMatrix(data=as.matrix(train), label=vector_status_group)


params = list(
  objective = "multi:softmax",
  num_class = 3,
  colsample_bytree  = 0.3,
  colsample_bylevel = 0.9,
  colsample_bynode  = 0.7,
  max_depth         = 15,
  eta               = 0.02
)

tic()
my_model <- fit_xgboost_model(params, train = xgb.train, val = xgb.train, nrounds = 600)
toc()

# accuracy = 1 - mlogloss
accuracy <- 1 - tail(my_model$evaluation_log$val1_mlogloss, 1)
accuracy

xgb_pred <- make_predictions_xgboost(my_model, test)

# Guardamos la submission
fwrite(xgb_pred, file = "final_submission.csv")
```

